{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "import tensorflow.keras\n",
    "import tensorflow as tf\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# imdb_reviews/submwords8k - 8000 vocab size\n",
    "# read train_data, test_data and info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "(train_data, test_data), info = tfds.load('imdb_reviews/subwords8k', split=(tfds.Split.TRAIN, tfds.Split.TEST), \n",
    "                                          with_info=True, as_supervised=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Type of 'info'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow_datasets.core.dataset_info.DatasetInfo"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is in 'info'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tfds.core.DatasetInfo(\n",
       "    name='imdb_reviews',\n",
       "    version=1.0.0,\n",
       "    description='Large Movie Review Dataset.\n",
       "This is a dataset for binary sentiment classification containing substantially more data than previous benchmark datasets. We provide a set of 25,000 highly polar movie reviews for training, and 25,000 for testing. There is additional unlabeled data for use as well.',\n",
       "    homepage='http://ai.stanford.edu/~amaas/data/sentiment/',\n",
       "    features=FeaturesDict({\n",
       "        'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=2),\n",
       "        'text': Text(shape=(None,), dtype=tf.int64, encoder=<SubwordTextEncoder vocab_size=8185>),\n",
       "    }),\n",
       "    total_num_examples=100000,\n",
       "    splits={\n",
       "        'test': 25000,\n",
       "        'train': 25000,\n",
       "        'unsupervised': 50000,\n",
       "    },\n",
       "    supervised_keys=('text', 'label'),\n",
       "    citation=\"\"\"@InProceedings{maas-EtAl:2011:ACL-HLT2011,\n",
       "      author    = {Maas, Andrew L.  and  Daly, Raymond E.  and  Pham, Peter T.  and  Huang, Dan  and  Ng, Andrew Y.  and  Potts, Christopher},\n",
       "      title     = {Learning Word Vectors for Sentiment Analysis},\n",
       "      booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},\n",
       "      month     = {June},\n",
       "      year      = {2011},\n",
       "      address   = {Portland, Oregon, USA},\n",
       "      publisher = {Association for Computational Linguistics},\n",
       "      pages     = {142--150},\n",
       "      url       = {http://www.aclweb.org/anthology/P11-1015}\n",
       "    }\"\"\",\n",
       "    redistribution_info=,\n",
       ")"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# info.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FeaturesDict({\n",
       "    'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=2),\n",
       "    'text': Text(shape=(None,), dtype=tf.int64, encoder=<SubwordTextEncoder vocab_size=8185>),\n",
       "})"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info.features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# info.features['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(shape=(None,), dtype=tf.int64, encoder=<SubwordTextEncoder vocab_size=8185>)"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info.features['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# info.features['text'].encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<SubwordTextEncoder vocab_size=8185>"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info.features['text'].encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = info.features['text'].encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# info.features['text'].encoder.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8185"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# info.features['text'].encoder.subwords - after tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the_',\n",
       " ', ',\n",
       " '. ',\n",
       " 'a_',\n",
       " 'and_',\n",
       " 'of_',\n",
       " 'to_',\n",
       " 's_',\n",
       " 'is_',\n",
       " 'br',\n",
       " 'in_',\n",
       " 'I_',\n",
       " 'that_',\n",
       " 'this_',\n",
       " 'it_',\n",
       " ' /><',\n",
       " ' />',\n",
       " 'was_',\n",
       " 'The_',\n",
       " 'as_',\n",
       " 't_',\n",
       " 'with_',\n",
       " 'for_',\n",
       " '.<',\n",
       " 'on_',\n",
       " 'but_',\n",
       " 'movie_',\n",
       " ' (',\n",
       " 'are_',\n",
       " 'his_',\n",
       " 'have_',\n",
       " 'film_',\n",
       " 'not_',\n",
       " 'ing_',\n",
       " 'be_',\n",
       " 'ed_',\n",
       " 'you_',\n",
       " ' \"',\n",
       " 'it',\n",
       " 'd_',\n",
       " 'an_',\n",
       " 'he_',\n",
       " 'by_',\n",
       " 'at_',\n",
       " 'one_',\n",
       " 'who_',\n",
       " 'y_',\n",
       " 'from_',\n",
       " 'e_',\n",
       " 'or_',\n",
       " 'all_',\n",
       " 'like_',\n",
       " 'they_',\n",
       " '\" ',\n",
       " 'so_',\n",
       " 'just_',\n",
       " 'has_',\n",
       " ') ',\n",
       " 'her_',\n",
       " 'about_',\n",
       " 'out_',\n",
       " 'This_',\n",
       " 'some_',\n",
       " 'ly_',\n",
       " 'movie',\n",
       " 'film',\n",
       " 'very_',\n",
       " 'more_',\n",
       " 'It_',\n",
       " 'would_',\n",
       " 'what_',\n",
       " 'when_',\n",
       " 'which_',\n",
       " 'good_',\n",
       " 'if_',\n",
       " 'up_',\n",
       " 'only_',\n",
       " 'even_',\n",
       " 'their_',\n",
       " 'had_',\n",
       " 'really_',\n",
       " 'my_',\n",
       " 'can_',\n",
       " 'no_',\n",
       " 'were_',\n",
       " 'see_',\n",
       " 'she_',\n",
       " '? ',\n",
       " 'than_',\n",
       " '! ',\n",
       " 'there_',\n",
       " 'get_',\n",
       " 'been_',\n",
       " 'into_',\n",
       " ' - ',\n",
       " 'will_',\n",
       " 'much_',\n",
       " 'story_',\n",
       " 'because_',\n",
       " 'ing',\n",
       " 'time_',\n",
       " 'n_',\n",
       " 'we_',\n",
       " 'ed',\n",
       " 'me_',\n",
       " ': ',\n",
       " 'most_',\n",
       " 'other_',\n",
       " 'don',\n",
       " 'do_',\n",
       " 'm_',\n",
       " 'es_',\n",
       " 'how_',\n",
       " 'also_',\n",
       " 'make_',\n",
       " 'its_',\n",
       " 'could_',\n",
       " 'first_',\n",
       " 'any_',\n",
       " \"' \",\n",
       " 'people_',\n",
       " 'great_',\n",
       " 've_',\n",
       " 'ly',\n",
       " 'er_',\n",
       " 'made_',\n",
       " 'r_',\n",
       " 'But_',\n",
       " 'think_',\n",
       " \" '\",\n",
       " 'i_',\n",
       " 'bad_',\n",
       " 'A_',\n",
       " 'And_',\n",
       " 'It',\n",
       " 'on',\n",
       " '; ',\n",
       " 'him_',\n",
       " 'being_',\n",
       " 'never_',\n",
       " 'way_',\n",
       " 'that',\n",
       " 'many_',\n",
       " 'then_',\n",
       " 'where_',\n",
       " 'two_',\n",
       " 'In_',\n",
       " 'after_',\n",
       " 'too_',\n",
       " 'little_',\n",
       " 'you',\n",
       " '), ',\n",
       " 'well_',\n",
       " 'ng_',\n",
       " 'your_',\n",
       " 'If_',\n",
       " 'l_',\n",
       " '). ',\n",
       " 'does_',\n",
       " 'ever_',\n",
       " 'them_',\n",
       " 'did_',\n",
       " 'watch_',\n",
       " 'know_',\n",
       " 'seen_',\n",
       " 'time',\n",
       " 'er',\n",
       " 'character_',\n",
       " 'over_',\n",
       " 'characters_',\n",
       " 'movies_',\n",
       " 'man_',\n",
       " 'There_',\n",
       " 'love_',\n",
       " 'best_',\n",
       " 'still_',\n",
       " 'off_',\n",
       " 'such_',\n",
       " 'in',\n",
       " 'should_',\n",
       " 'the',\n",
       " 're_',\n",
       " 'He_',\n",
       " 'plot_',\n",
       " 'films_',\n",
       " 'go_',\n",
       " 'these_',\n",
       " 'acting_',\n",
       " 'doesn',\n",
       " 'es',\n",
       " 'show_',\n",
       " 'through_',\n",
       " 'better_',\n",
       " 'al_',\n",
       " 'something_',\n",
       " 'didn',\n",
       " 'back_',\n",
       " 'those_',\n",
       " 'us_',\n",
       " 'less_',\n",
       " '...',\n",
       " 'say_',\n",
       " 'is',\n",
       " 'one',\n",
       " 'makes_',\n",
       " 'and',\n",
       " 'can',\n",
       " 'all',\n",
       " 'ion_',\n",
       " 'find_',\n",
       " 'scene_',\n",
       " 'old_',\n",
       " 'real_',\n",
       " 'few_',\n",
       " 'going_',\n",
       " 'well',\n",
       " 'actually_',\n",
       " 'watching_',\n",
       " 'life_',\n",
       " 'me',\n",
       " '. <',\n",
       " 'o_',\n",
       " 'man',\n",
       " 'there',\n",
       " 'scenes_',\n",
       " 'same_',\n",
       " 'he',\n",
       " 'end_',\n",
       " 'this',\n",
       " '... ',\n",
       " 'k_',\n",
       " 'while_',\n",
       " 'thing_',\n",
       " 'of',\n",
       " 'look_',\n",
       " 'quite_',\n",
       " 'out',\n",
       " 'lot_',\n",
       " 'want_',\n",
       " 'why_',\n",
       " 'seems_',\n",
       " 'every_',\n",
       " 'll_',\n",
       " 'pretty_',\n",
       " 'got_',\n",
       " 'able_',\n",
       " 'nothing_',\n",
       " 'good',\n",
       " 'As_',\n",
       " 'story',\n",
       " ' & ',\n",
       " 'another_',\n",
       " 'take_',\n",
       " 'to',\n",
       " 'years_',\n",
       " 'between_',\n",
       " 'give_',\n",
       " 'am_',\n",
       " 'work_',\n",
       " 'isn',\n",
       " 'part_',\n",
       " 'before_',\n",
       " 'actors_',\n",
       " 'may_',\n",
       " 'gets_',\n",
       " 'young_',\n",
       " 'down_',\n",
       " 'around_',\n",
       " 'ng',\n",
       " 'thought_',\n",
       " 'though_',\n",
       " 'end',\n",
       " 'without_',\n",
       " 'What_',\n",
       " 'They_',\n",
       " 'things_',\n",
       " 'life',\n",
       " 'always_',\n",
       " 'must_',\n",
       " 'cast_',\n",
       " 'almost_',\n",
       " 'h_',\n",
       " '10',\n",
       " 'saw_',\n",
       " 'own_',\n",
       " 'here',\n",
       " 'bit_',\n",
       " 'come_',\n",
       " 'both_',\n",
       " 'might_',\n",
       " 'g_',\n",
       " 'whole_',\n",
       " 'new_',\n",
       " 'director_',\n",
       " 'them',\n",
       " 'horror_',\n",
       " 'ce',\n",
       " 'You_',\n",
       " 'least_',\n",
       " 'bad',\n",
       " 'big_',\n",
       " 'enough_',\n",
       " 'him',\n",
       " 'feel_',\n",
       " 'probably_',\n",
       " 'up',\n",
       " 'here_',\n",
       " 'making_',\n",
       " 'long_',\n",
       " 'her',\n",
       " 'st_',\n",
       " 'kind_',\n",
       " '--',\n",
       " 'original_',\n",
       " 'fact_',\n",
       " 'rather_',\n",
       " 'or',\n",
       " 'far_',\n",
       " 'nt_',\n",
       " 'played_',\n",
       " 'found_',\n",
       " 'last_',\n",
       " 'movies',\n",
       " 'When_',\n",
       " 'so',\n",
       " '\", ',\n",
       " 'comes_',\n",
       " 'action_',\n",
       " 'She_',\n",
       " 've',\n",
       " 'our_',\n",
       " 'anything_',\n",
       " 'funny_',\n",
       " 'ion',\n",
       " 'right_',\n",
       " 'way',\n",
       " 'trying_',\n",
       " 'now_',\n",
       " 'ous_',\n",
       " 'each_',\n",
       " 'done_',\n",
       " 'since_',\n",
       " 'ic_',\n",
       " 'point_',\n",
       " '\". ',\n",
       " 'wasn',\n",
       " 'interesting_',\n",
       " 'c_',\n",
       " 'worst_',\n",
       " 'te_',\n",
       " 'le_',\n",
       " 'ble_',\n",
       " 'ty_',\n",
       " 'looks_',\n",
       " 'show',\n",
       " 'put_',\n",
       " 'looking_',\n",
       " 'especially_',\n",
       " 'believe_',\n",
       " 'en_',\n",
       " 'goes_',\n",
       " 'over',\n",
       " 'ce_',\n",
       " 'p_',\n",
       " 'films',\n",
       " 'hard_',\n",
       " 'main_',\n",
       " 'be',\n",
       " 'having_',\n",
       " 'ry',\n",
       " 'TV_',\n",
       " 'worth_',\n",
       " 'One_',\n",
       " 'do',\n",
       " 'al',\n",
       " 're',\n",
       " 'again',\n",
       " 'series_',\n",
       " 'takes_',\n",
       " 'guy_',\n",
       " 'family_',\n",
       " 'seem_',\n",
       " 'plays_',\n",
       " 'role_',\n",
       " 'away_',\n",
       " 'world_',\n",
       " 'My_',\n",
       " 'character',\n",
       " ', \"',\n",
       " 'performance_',\n",
       " '2_',\n",
       " 'So_',\n",
       " 'watched_',\n",
       " 'John_',\n",
       " 'th_',\n",
       " 'plot',\n",
       " 'script_',\n",
       " 'For_',\n",
       " 'sure_',\n",
       " 'characters',\n",
       " 'set_',\n",
       " 'different_',\n",
       " 'minutes_',\n",
       " 'All_',\n",
       " 'American_',\n",
       " 'anyone_',\n",
       " 'Not_',\n",
       " 'music_',\n",
       " 'ry_',\n",
       " 'shows_',\n",
       " 'too',\n",
       " 'son_',\n",
       " 'en',\n",
       " 'day_',\n",
       " 'use_',\n",
       " 'someone_',\n",
       " 'for',\n",
       " 'woman_',\n",
       " 'yet_',\n",
       " '.\" ',\n",
       " 'during_',\n",
       " 'she',\n",
       " 'ro',\n",
       " '- ',\n",
       " 'times_',\n",
       " 'left_',\n",
       " 'used_',\n",
       " 'le',\n",
       " 'three_',\n",
       " 'play_',\n",
       " 'work',\n",
       " 'ness_',\n",
       " 'We_',\n",
       " 'girl_',\n",
       " 'comedy_',\n",
       " 'ment_',\n",
       " 'an',\n",
       " 'simply_',\n",
       " 'off',\n",
       " 'ies_',\n",
       " 'funny',\n",
       " 'ne',\n",
       " 'acting',\n",
       " 'That_',\n",
       " 'fun_',\n",
       " 'completely_',\n",
       " 'st',\n",
       " 'seeing_',\n",
       " 'us',\n",
       " 'te',\n",
       " 'special_',\n",
       " 'ation_',\n",
       " 'as',\n",
       " 'ive_',\n",
       " 'ful_',\n",
       " 'read_',\n",
       " 'reason_',\n",
       " 'co',\n",
       " 'need_',\n",
       " 'sa',\n",
       " 'true_',\n",
       " 'ted_',\n",
       " 'like',\n",
       " 'ck',\n",
       " 'place_',\n",
       " 'they',\n",
       " '10_',\n",
       " 'However',\n",
       " 'until_',\n",
       " 'rest_',\n",
       " 'sense_',\n",
       " 'ity_',\n",
       " 'everything_',\n",
       " 'people',\n",
       " 'nt',\n",
       " 'ending_',\n",
       " 'again_',\n",
       " 'ers_',\n",
       " 'given_',\n",
       " 'idea_',\n",
       " 'let_',\n",
       " 'nice_',\n",
       " 'help_',\n",
       " 'no',\n",
       " 'truly_',\n",
       " 'beautiful_',\n",
       " 'ter',\n",
       " 'ck_',\n",
       " 'version_',\n",
       " 'try_',\n",
       " 'came_',\n",
       " 'Even_',\n",
       " 'DVD_',\n",
       " 'se',\n",
       " 'mis',\n",
       " 'scene',\n",
       " 'job_',\n",
       " 'ting_',\n",
       " 'Me',\n",
       " 'At_',\n",
       " 'who',\n",
       " 'money_',\n",
       " 'ment',\n",
       " 'ch',\n",
       " 'recommend_',\n",
       " 'was',\n",
       " 'once_',\n",
       " 'getting_',\n",
       " 'tell_',\n",
       " 'de_',\n",
       " 'gives_',\n",
       " 'not',\n",
       " 'Lo',\n",
       " 'we',\n",
       " 'son',\n",
       " 'shot_',\n",
       " 'second_',\n",
       " 'After_',\n",
       " 'To_',\n",
       " 'high_',\n",
       " 'screen_',\n",
       " ' -- ',\n",
       " 'keep_',\n",
       " 'felt_',\n",
       " 'with',\n",
       " 'great',\n",
       " 'everyone_',\n",
       " 'although_',\n",
       " 'poor_',\n",
       " 'el',\n",
       " 'half_',\n",
       " 'playing_',\n",
       " 'couple_',\n",
       " 'now',\n",
       " 'ble',\n",
       " 'excellent_',\n",
       " 'enjoy_',\n",
       " 'couldn',\n",
       " 'x_',\n",
       " 'ne_',\n",
       " ',\" ',\n",
       " 'ie_',\n",
       " 'go',\n",
       " 'become_',\n",
       " 'less',\n",
       " 'himself_',\n",
       " 'supposed_',\n",
       " 'won',\n",
       " 'understand_',\n",
       " 'seen',\n",
       " 'ally_',\n",
       " 'THE_',\n",
       " 'se_',\n",
       " 'actor_',\n",
       " 'ts_',\n",
       " 'small_',\n",
       " 'line_',\n",
       " 'na',\n",
       " 'audience_',\n",
       " 'fan_',\n",
       " 'et',\n",
       " 'world',\n",
       " 'entire_',\n",
       " 'said_',\n",
       " 'at',\n",
       " '3_',\n",
       " 'scenes',\n",
       " 'rs_',\n",
       " 'full_',\n",
       " 'year_',\n",
       " 'men_',\n",
       " 'ke',\n",
       " 'doing_',\n",
       " 'went_',\n",
       " 'director',\n",
       " 'back',\n",
       " 'early_',\n",
       " 'Hollywood_',\n",
       " 'start_',\n",
       " 'liked_',\n",
       " 'against_',\n",
       " 'remember_',\n",
       " 'love',\n",
       " 'He',\n",
       " 'along_',\n",
       " 'ic',\n",
       " 'His_',\n",
       " 'wife_',\n",
       " 'effects_',\n",
       " 'together_',\n",
       " 'ch_',\n",
       " 'Ra',\n",
       " 'ty',\n",
       " 'maybe_',\n",
       " 'age',\n",
       " 'S_',\n",
       " 'While_',\n",
       " 'often_',\n",
       " 'sort_',\n",
       " 'definitely_',\n",
       " 'No',\n",
       " 'script',\n",
       " 'times',\n",
       " 'absolutely_',\n",
       " 'book_',\n",
       " 'day',\n",
       " 'human_',\n",
       " 'There',\n",
       " 'top_',\n",
       " 'ta',\n",
       " 'becomes_',\n",
       " 'piece_',\n",
       " 'waste_',\n",
       " 'seemed_',\n",
       " 'down',\n",
       " '5_',\n",
       " 'later_',\n",
       " 'rs',\n",
       " 'ja',\n",
       " 'certainly_',\n",
       " 'budget_',\n",
       " 'th',\n",
       " 'nce_',\n",
       " '200',\n",
       " '. (',\n",
       " 'age_',\n",
       " 'next_',\n",
       " 'ar',\n",
       " 'several_',\n",
       " 'ling_',\n",
       " 'short_',\n",
       " 'sh',\n",
       " 'fe',\n",
       " 'Of_',\n",
       " 'instead_',\n",
       " 'Man',\n",
       " 'T_',\n",
       " 'right',\n",
       " 'father_',\n",
       " 'actors',\n",
       " 'wanted_',\n",
       " 'cast',\n",
       " 'black_',\n",
       " 'Don',\n",
       " 'more',\n",
       " '1_',\n",
       " 'comedy',\n",
       " 'better',\n",
       " 'camera_',\n",
       " 'wonderful_',\n",
       " 'production_',\n",
       " 'inter',\n",
       " 'course',\n",
       " 'low_',\n",
       " 'else_',\n",
       " 'w_',\n",
       " 'ness',\n",
       " 'course_',\n",
       " 'based_',\n",
       " 'ti',\n",
       " 'Some_',\n",
       " 'know',\n",
       " 'house_',\n",
       " 'say',\n",
       " 'de',\n",
       " 'watch',\n",
       " 'ous',\n",
       " 'pro',\n",
       " 'tries_',\n",
       " 'ra',\n",
       " 'kids_',\n",
       " 'etc',\n",
       " ' \\x96 ',\n",
       " 'loved_',\n",
       " 'est_',\n",
       " 'fun',\n",
       " 'made',\n",
       " 'video_',\n",
       " 'un',\n",
       " 'totally_',\n",
       " 'Michael_',\n",
       " 'ho',\n",
       " 'mind_',\n",
       " 'No_',\n",
       " 'Be',\n",
       " 'ive',\n",
       " 'La',\n",
       " 'Fi',\n",
       " 'du',\n",
       " 'ers',\n",
       " 'Well',\n",
       " 'wants_',\n",
       " 'How_',\n",
       " 'series',\n",
       " 'performances_',\n",
       " 'written_',\n",
       " 'live_',\n",
       " 'New_',\n",
       " 'So',\n",
       " 'Ne',\n",
       " 'Na',\n",
       " 'night_',\n",
       " 'ge',\n",
       " 'gave_',\n",
       " 'home_',\n",
       " 'heart',\n",
       " 'women_',\n",
       " 'nu',\n",
       " 'ss_',\n",
       " 'hope_',\n",
       " 'ci',\n",
       " 'friends_',\n",
       " 'Se',\n",
       " 'years',\n",
       " 'sub',\n",
       " 'head_',\n",
       " 'Y_',\n",
       " 'Du',\n",
       " '. \"',\n",
       " 'turn_',\n",
       " 'red_',\n",
       " 'perfect_',\n",
       " 'already_',\n",
       " 'classic_',\n",
       " 'tri',\n",
       " 'ss',\n",
       " 'person_',\n",
       " 'star_',\n",
       " 'screen',\n",
       " 'style_',\n",
       " 'ur',\n",
       " 'starts_',\n",
       " 'under_',\n",
       " 'Then_',\n",
       " 'ke_',\n",
       " 'ine',\n",
       " 'ies',\n",
       " 'um',\n",
       " 'ie',\n",
       " 'face_',\n",
       " 'ir',\n",
       " 'enjoyed_',\n",
       " 'point',\n",
       " 'lines_',\n",
       " 'Mr',\n",
       " 'turns_',\n",
       " 'what',\n",
       " 'side_',\n",
       " 'sex_',\n",
       " 'Ha',\n",
       " 'final_',\n",
       " ').<',\n",
       " 'With_',\n",
       " 'care_',\n",
       " 'tion_',\n",
       " 'She',\n",
       " 'ation',\n",
       " 'Ar',\n",
       " 'ma',\n",
       " 'problem_',\n",
       " 'lost_',\n",
       " 'are',\n",
       " 'li',\n",
       " '4_',\n",
       " 'fully_',\n",
       " 'oo',\n",
       " 'sha',\n",
       " 'Just_',\n",
       " 'name_',\n",
       " 'ina',\n",
       " 'boy_',\n",
       " 'finally_',\n",
       " 'ol',\n",
       " '!<',\n",
       " 'Bo',\n",
       " 'about',\n",
       " 'though',\n",
       " 'hand',\n",
       " 'ton',\n",
       " 'lead_',\n",
       " 'school_',\n",
       " 'ns',\n",
       " 'ha',\n",
       " 'favorite_',\n",
       " 'stupid_',\n",
       " 'gi',\n",
       " 'original',\n",
       " 'mean_',\n",
       " 'To',\n",
       " 'took_',\n",
       " 'either_',\n",
       " 'ni',\n",
       " 'book',\n",
       " 'episode_',\n",
       " 'om',\n",
       " 'Su',\n",
       " 'D_',\n",
       " 'Mc',\n",
       " 'house',\n",
       " 'cannot_',\n",
       " 'stars_',\n",
       " 'behind_',\n",
       " 'see',\n",
       " 'other',\n",
       " 'Che',\n",
       " 'role',\n",
       " 'art',\n",
       " 'ever',\n",
       " 'Why_',\n",
       " 'father',\n",
       " 'case_',\n",
       " 'tic_',\n",
       " 'moments_',\n",
       " 'Co',\n",
       " 'works_',\n",
       " 'sound_',\n",
       " 'Ta',\n",
       " 'guess_',\n",
       " 'perhaps_',\n",
       " 'Vi',\n",
       " 'thing',\n",
       " 'fine_',\n",
       " 'fact',\n",
       " 'music',\n",
       " 'non',\n",
       " 'ful',\n",
       " 'action',\n",
       " 'ity',\n",
       " 'ct',\n",
       " 'ate_',\n",
       " 'type_',\n",
       " 'lack_',\n",
       " 'death_',\n",
       " 'art_',\n",
       " 'able',\n",
       " 'Ja',\n",
       " 'ge_',\n",
       " 'wouldn',\n",
       " 'am',\n",
       " 'tor',\n",
       " 'extremely_',\n",
       " 'pre',\n",
       " 'self',\n",
       " 'Mor',\n",
       " 'particularly_',\n",
       " 'bo',\n",
       " 'est',\n",
       " 'Ba',\n",
       " 'ya',\n",
       " 'play',\n",
       " 'Pa',\n",
       " 'ther',\n",
       " 'heard_',\n",
       " 'however',\n",
       " 'ver',\n",
       " 'dy_',\n",
       " 'Sa',\n",
       " 'ding_',\n",
       " 'led_',\n",
       " 'late_',\n",
       " 'feeling_',\n",
       " 'per',\n",
       " 'low',\n",
       " 'ably_',\n",
       " 'Un',\n",
       " 'On_',\n",
       " 'known_',\n",
       " 'kill_',\n",
       " 'fight_',\n",
       " 'beginning_',\n",
       " 'cat',\n",
       " 'bit',\n",
       " 'title_',\n",
       " 'vo',\n",
       " 'short',\n",
       " 'old',\n",
       " 'including_',\n",
       " 'Da',\n",
       " 'coming_',\n",
       " 'That',\n",
       " 'place',\n",
       " 'looked_',\n",
       " 'best',\n",
       " 'Lu',\n",
       " 'ent_',\n",
       " 'bla',\n",
       " 'quality_',\n",
       " 'except_',\n",
       " '...<',\n",
       " 'ff',\n",
       " 'decent_',\n",
       " 'much',\n",
       " 'De',\n",
       " 'Bu',\n",
       " 'ter_',\n",
       " 'attempt_',\n",
       " 'Bi',\n",
       " 'taking_',\n",
       " 'ig',\n",
       " 'Ti',\n",
       " 'whose_',\n",
       " 'dialogue_',\n",
       " 'zz',\n",
       " 'war_',\n",
       " 'ill',\n",
       " 'Te',\n",
       " 'war',\n",
       " 'Hu',\n",
       " 'James_',\n",
       " '..',\n",
       " 'under',\n",
       " 'ring_',\n",
       " 'pa',\n",
       " 'ot',\n",
       " 'expect_',\n",
       " 'Ga',\n",
       " 'itself_',\n",
       " 'line',\n",
       " 'lives_',\n",
       " 'let',\n",
       " 'Dr',\n",
       " 'mp',\n",
       " 'che',\n",
       " 'mean',\n",
       " 'called_',\n",
       " 'complete_',\n",
       " 'terrible_',\n",
       " 'boring_',\n",
       " 'others_',\n",
       " '\" (',\n",
       " 'aren',\n",
       " 'star',\n",
       " 'long',\n",
       " 'Li',\n",
       " 'mother_',\n",
       " 'si',\n",
       " 'highly_',\n",
       " 'ab',\n",
       " 'ex',\n",
       " 'os',\n",
       " 'nd',\n",
       " 'ten_',\n",
       " 'ten',\n",
       " 'run_',\n",
       " 'directed_',\n",
       " 'town_',\n",
       " 'friend_',\n",
       " 'David_',\n",
       " 'taken_',\n",
       " 'finds_',\n",
       " 'fans_',\n",
       " 'Mar',\n",
       " 'writing_',\n",
       " 'white_',\n",
       " 'u_',\n",
       " 'obviously_',\n",
       " 'mar',\n",
       " 'Ho',\n",
       " 'year',\n",
       " 'stop_',\n",
       " 'f_',\n",
       " 'leave_',\n",
       " 'king_',\n",
       " 'act_',\n",
       " 'mind',\n",
       " 'entertaining_',\n",
       " 'ish_',\n",
       " 'Ka',\n",
       " 'throughout_',\n",
       " 'viewer_',\n",
       " 'despite_',\n",
       " 'Robert_',\n",
       " 'somewhat_',\n",
       " 'hour_',\n",
       " 'car_',\n",
       " 'evil_',\n",
       " 'Although_',\n",
       " 'wrong_',\n",
       " 'Ro',\n",
       " 'dead_',\n",
       " 'body_',\n",
       " 'awful_',\n",
       " 'home',\n",
       " 'exactly_',\n",
       " 'bi',\n",
       " 'family',\n",
       " 'ts',\n",
       " 'usually_',\n",
       " 'told_',\n",
       " 'z_',\n",
       " 'oc',\n",
       " 'minutes',\n",
       " 'tra',\n",
       " 'some',\n",
       " 'actor',\n",
       " 'den',\n",
       " 'but',\n",
       " 'Sha',\n",
       " 'tu',\n",
       " 'strong_',\n",
       " ...]"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info.features['text'].encoder.subwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# type of 'train_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.data.ops.dataset_ops.DatasetV1Adapter"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# see what is in train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[  62    9    4  301 4161  267  148    1 3240 1779  787    3   62 2315\n",
      "  260 7968   21 1240   20  445   20  261  204    2    5   15  635 7742\n",
      "  149   97  101   25  184 4127    3 4666 7913  690   25    9  176    1\n",
      "  175  233   60   14  694    2   26   30 1858 3162   34    9 3636   40\n",
      "  267   11   14 2362 8050    3   19  695   29   51 1816 7138    2   26\n",
      "   14  101    1  397 3953  199  615    3   19  328    9 3362 4712 7961\n",
      "    5    1  184    9   77 4167   64 1152    2   55   12  459 1461    6\n",
      " 1713 3326   11 1147    7 1464 5691 7961  421 8026   38 1746 1074  618\n",
      "   54   65    3 1987    2    1 3326   29  214    5  318 2338 3960 8039\n",
      "    2    5  325    2   14   32    9 4157 7961   44  883 7975], shape=(138,), dtype=int64)\n",
      "tf.Tensor(0, shape=(), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "for x,y in train_data:\n",
    "    \n",
    "    print(x)\n",
    "    print(y)\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# types contained in train_data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n"
     ]
    }
   ],
   "source": [
    "for x,y in train_data:\n",
    "    \n",
    "    print(type(x))\n",
    "    print(type(y))\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# this is how to take the array in an EagerTensor element in train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  62    9    4  301 4161  267  148    1 3240 1779  787    3   62 2315\n",
      "  260 7968   21 1240   20  445   20  261  204    2    5   15  635 7742\n",
      "  149   97  101   25  184 4127    3 4666 7913  690   25    9  176    1\n",
      "  175  233   60   14  694    2   26   30 1858 3162   34    9 3636   40\n",
      "  267   11   14 2362 8050    3   19  695   29   51 1816 7138    2   26\n",
      "   14  101    1  397 3953  199  615    3   19  328    9 3362 4712 7961\n",
      "    5    1  184    9   77 4167   64 1152    2   55   12  459 1461    6\n",
      " 1713 3326   11 1147    7 1464 5691 7961  421 8026   38 1746 1074  618\n",
      "   54   65    3 1987    2    1 3326   29  214    5  318 2338 3960 8039\n",
      "    2    5  325    2   14   32    9 4157 7961   44  883 7975]\n"
     ]
    }
   ],
   "source": [
    "for x,y in train_data:\n",
    "    \n",
    "    print(x.numpy())\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# take the text sequences (integer encoded sequences) in train_data into a list\n",
    "# also take the labels into a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text_sequences = list()\n",
    "train_labels = list()\n",
    "for x,y in train_data:\n",
    "    train_text_sequences.append(x.numpy())\n",
    "    train_labels.append(y.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# take the text sequences list and labels list into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "dct_train_labeled_text = {'text sequences': train_text_sequences, 'labels': train_labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_labeled_text = pd.DataFrame(dct_train_labeled_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text sequences</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>[62, 9, 4, 301, 4161, 267, 148, 1, 3240, 1779,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>[2130, 99, 12, 18, 55, 2554, 2, 3508, 5, 7995,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>[4491, 40, 6, 1, 7450, 34, 4798, 80, 4, 238, 7...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>[398, 105, 14, 9, 4, 98, 13, 732, 22, 63, 333,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>[62, 9, 33, 4, 132, 65, 3, 69, 2494, 1, 293, 5...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      text sequences  labels\n",
       "0  [62, 9, 4, 301, 4161, 267, 148, 1, 3240, 1779,...       0\n",
       "1  [2130, 99, 12, 18, 55, 2554, 2, 3508, 5, 7995,...       0\n",
       "2  [4491, 40, 6, 1, 7450, 34, 4798, 80, 4, 238, 7...       1\n",
       "3  [398, 105, 14, 9, 4, 98, 13, 732, 22, 63, 333,...       0\n",
       "4  [62, 9, 33, 4, 132, 65, 3, 69, 2494, 1, 293, 5...       1"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_labeled_text.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# the text sequences are of variable length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_of_train_text = list()\n",
    "for i,r in df_train_labeled_text.iterrows():\n",
    "    length_of_train_text.append(len(r['text sequences']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[138, 200, 708, 146, 126]"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "length_of_train_text[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3944"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(length_of_train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(length_of_train_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# take the text sequences and labels in the test_data into lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text_sequences = list()\n",
    "test_labels = list()\n",
    "for x,y in test_data:\n",
    "    test_text_sequences.append(x.numpy())\n",
    "    test_labels.append(y.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# convert these lists into a dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "dct_test_labeled_text = {'text sequences': test_text_sequences, 'labels': test_labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_labeled_text = pd.DataFrame(dct_test_labeled_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text sequences</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>[69, 5680, 22, 155, 6819, 7961, 6197, 309, 215...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>[12, 582, 448, 14, 44, 82, 1080, 2667, 464, 44...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>[62, 631, 32, 620, 3783, 8, 84, 3877, 190, 3, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>[6270, 762, 21, 5290, 6724, 3077, 8, 11, 59, 4...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>[8002, 7968, 111, 81, 33, 215, 7, 613, 82, 101...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      text sequences  labels\n",
       "0  [69, 5680, 22, 155, 6819, 7961, 6197, 309, 215...       1\n",
       "1  [12, 582, 448, 14, 44, 82, 1080, 2667, 464, 44...       0\n",
       "2  [62, 631, 32, 620, 3783, 8, 84, 3877, 190, 3, ...       1\n",
       "3  [6270, 762, 21, 5290, 6724, 3077, 8, 11, 59, 4...       0\n",
       "4  [8002, 7968, 111, 81, 33, 215, 7, 613, 82, 101...       0"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_labeled_text.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# length of text sequences in test_data are of variable length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_of_test_text = list()\n",
    "for i,r in df_test_labeled_text.iterrows():\n",
    "    length_of_test_text.append(len(r['text sequences']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[226, 248, 203, 163, 159]"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "length_of_test_text[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3454"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(length_of_test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(length_of_test_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# take the train text sequences and test text sequences as lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = list(df_train_labeled_text['text sequences'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = list(df_test_labeled_text['text sequences'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pad the text sequences\n",
    "# add 0s to text sequences having length less than the max length\n",
    "# truncate the text sequences having length more than the max length to max length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 100\n",
    "\n",
    "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
    "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# after padding see the lengths of text sequences in train and test data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_post_padding_length = list()\n",
    "for a in X_train:\n",
    "    train_post_padding_length.append(len(a))\n",
    "    \n",
    "test_post_padding_length = list()\n",
    "for a in X_test:\n",
    "    test_post_padding_length.append(len(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[100, 100, 100, 100, 100]"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_post_padding_length[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[100, 100, 100, 100, 100]"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_post_padding_length[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build the neural network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (None, None, 100)         818500    \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_6 ( (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 16)                1616      \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 820,133\n",
      "Trainable params: 820,133\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embedding_dim=16\n",
    "\n",
    "model = keras.Sequential([\n",
    "  layers.Embedding(encoder.vocab_size, embedding_dim),\n",
    "  layers.GlobalAveragePooling1D(),\n",
    "  layers.Dense(16, activation='relu'),\n",
    "  layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# take the train labels as a numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = list(df_train_labeled_text['labels'])\n",
    "y_train = np.array(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# take the test labels as a numpy array "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = list(df_test_labeled_text['labels'])\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples\n",
      "Epoch 1/5\n",
      "25000/25000 [==============================] - ETA: 4:47 - loss: 0.6936 - accuracy: 0.43 - ETA: 1:06 - loss: 0.6930 - accuracy: 0.44 - ETA: 34s - loss: 0.6928 - accuracy: 0.4659 - ETA: 27s - loss: 0.6924 - accuracy: 0.464 - ETA: 25s - loss: 0.6924 - accuracy: 0.470 - ETA: 20s - loss: 0.6923 - accuracy: 0.479 - ETA: 18s - loss: 0.6921 - accuracy: 0.480 - ETA: 16s - loss: 0.6919 - accuracy: 0.484 - ETA: 15s - loss: 0.6918 - accuracy: 0.484 - ETA: 14s - loss: 0.6915 - accuracy: 0.486 - ETA: 13s - loss: 0.6914 - accuracy: 0.492 - ETA: 12s - loss: 0.6911 - accuracy: 0.493 - ETA: 12s - loss: 0.6909 - accuracy: 0.492 - ETA: 11s - loss: 0.6905 - accuracy: 0.497 - ETA: 11s - loss: 0.6902 - accuracy: 0.495 - ETA: 10s - loss: 0.6896 - accuracy: 0.494 - ETA: 10s - loss: 0.6890 - accuracy: 0.493 - ETA: 10s - loss: 0.6884 - accuracy: 0.492 - ETA: 9s - loss: 0.6875 - accuracy: 0.488 - ETA: 9s - loss: 0.6868 - accuracy: 0.48 - ETA: 9s - loss: 0.6860 - accuracy: 0.48 - ETA: 9s - loss: 0.6849 - accuracy: 0.48 - ETA: 8s - loss: 0.6837 - accuracy: 0.49 - ETA: 8s - loss: 0.6823 - accuracy: 0.49 - ETA: 8s - loss: 0.6809 - accuracy: 0.49 - ETA: 8s - loss: 0.6793 - accuracy: 0.49 - ETA: 8s - loss: 0.6773 - accuracy: 0.49 - ETA: 8s - loss: 0.6759 - accuracy: 0.49 - ETA: 7s - loss: 0.6741 - accuracy: 0.49 - ETA: 7s - loss: 0.6719 - accuracy: 0.49 - ETA: 7s - loss: 0.6697 - accuracy: 0.49 - ETA: 7s - loss: 0.6673 - accuracy: 0.50 - ETA: 7s - loss: 0.6648 - accuracy: 0.50 - ETA: 7s - loss: 0.6622 - accuracy: 0.51 - ETA: 7s - loss: 0.6600 - accuracy: 0.51 - ETA: 6s - loss: 0.6574 - accuracy: 0.51 - ETA: 6s - loss: 0.6548 - accuracy: 0.52 - ETA: 6s - loss: 0.6519 - accuracy: 0.52 - ETA: 6s - loss: 0.6488 - accuracy: 0.53 - ETA: 6s - loss: 0.6460 - accuracy: 0.53 - ETA: 6s - loss: 0.6430 - accuracy: 0.53 - ETA: 6s - loss: 0.6401 - accuracy: 0.54 - ETA: 6s - loss: 0.6372 - accuracy: 0.54 - ETA: 6s - loss: 0.6341 - accuracy: 0.55 - ETA: 6s - loss: 0.6320 - accuracy: 0.55 - ETA: 6s - loss: 0.6297 - accuracy: 0.55 - ETA: 5s - loss: 0.6265 - accuracy: 0.56 - ETA: 5s - loss: 0.6232 - accuracy: 0.56 - ETA: 5s - loss: 0.6205 - accuracy: 0.57 - ETA: 5s - loss: 0.6178 - accuracy: 0.57 - ETA: 5s - loss: 0.6158 - accuracy: 0.57 - ETA: 5s - loss: 0.6126 - accuracy: 0.58 - ETA: 5s - loss: 0.6098 - accuracy: 0.58 - ETA: 5s - loss: 0.6065 - accuracy: 0.59 - ETA: 5s - loss: 0.6042 - accuracy: 0.59 - ETA: 5s - loss: 0.6013 - accuracy: 0.59 - ETA: 5s - loss: 0.5977 - accuracy: 0.60 - ETA: 5s - loss: 0.5944 - accuracy: 0.60 - ETA: 5s - loss: 0.5909 - accuracy: 0.60 - ETA: 4s - loss: 0.5893 - accuracy: 0.61 - ETA: 4s - loss: 0.5873 - accuracy: 0.61 - ETA: 4s - loss: 0.5849 - accuracy: 0.61 - ETA: 4s - loss: 0.5822 - accuracy: 0.61 - ETA: 4s - loss: 0.5793 - accuracy: 0.62 - ETA: 4s - loss: 0.5769 - accuracy: 0.62 - ETA: 4s - loss: 0.5747 - accuracy: 0.62 - ETA: 4s - loss: 0.5717 - accuracy: 0.63 - ETA: 4s - loss: 0.5688 - accuracy: 0.63 - ETA: 4s - loss: 0.5668 - accuracy: 0.63 - ETA: 4s - loss: 0.5652 - accuracy: 0.63 - ETA: 4s - loss: 0.5638 - accuracy: 0.64 - ETA: 4s - loss: 0.5619 - accuracy: 0.64 - ETA: 4s - loss: 0.5596 - accuracy: 0.64 - ETA: 3s - loss: 0.5578 - accuracy: 0.64 - ETA: 3s - loss: 0.5554 - accuracy: 0.64 - ETA: 3s - loss: 0.5534 - accuracy: 0.65 - ETA: 3s - loss: 0.5508 - accuracy: 0.65 - ETA: 3s - loss: 0.5485 - accuracy: 0.65 - ETA: 3s - loss: 0.5467 - accuracy: 0.65 - ETA: 3s - loss: 0.5449 - accuracy: 0.65 - ETA: 3s - loss: 0.5434 - accuracy: 0.66 - ETA: 3s - loss: 0.5424 - accuracy: 0.66 - ETA: 3s - loss: 0.5418 - accuracy: 0.66 - ETA: 3s - loss: 0.5410 - accuracy: 0.66 - ETA: 3s - loss: 0.5391 - accuracy: 0.66 - ETA: 3s - loss: 0.5379 - accuracy: 0.66 - ETA: 3s - loss: 0.5360 - accuracy: 0.67 - ETA: 2s - loss: 0.5346 - accuracy: 0.67 - ETA: 2s - loss: 0.5336 - accuracy: 0.67 - ETA: 2s - loss: 0.5321 - accuracy: 0.67 - ETA: 2s - loss: 0.5306 - accuracy: 0.67 - ETA: 2s - loss: 0.5299 - accuracy: 0.67 - ETA: 2s - loss: 0.5284 - accuracy: 0.67 - ETA: 2s - loss: 0.5267 - accuracy: 0.67 - ETA: 2s - loss: 0.5257 - accuracy: 0.68 - ETA: 2s - loss: 0.5253 - accuracy: 0.68 - ETA: 2s - loss: 0.5243 - accuracy: 0.68 - ETA: 2s - loss: 0.5228 - accuracy: 0.68 - ETA: 2s - loss: 0.5223 - accuracy: 0.68 - ETA: 2s - loss: 0.5214 - accuracy: 0.68 - ETA: 2s - loss: 0.5204 - accuracy: 0.68 - ETA: 2s - loss: 0.5191 - accuracy: 0.68 - ETA: 2s - loss: 0.5181 - accuracy: 0.68 - ETA: 2s - loss: 0.5172 - accuracy: 0.68 - ETA: 2s - loss: 0.5164 - accuracy: 0.69 - ETA: 2s - loss: 0.5151 - accuracy: 0.69 - ETA: 2s - loss: 0.5141 - accuracy: 0.69 - ETA: 1s - loss: 0.5130 - accuracy: 0.69 - ETA: 1s - loss: 0.5121 - accuracy: 0.69 - ETA: 1s - loss: 0.5106 - accuracy: 0.69 - ETA: 1s - loss: 0.5095 - accuracy: 0.69 - ETA: 1s - loss: 0.5087 - accuracy: 0.69 - ETA: 1s - loss: 0.5080 - accuracy: 0.69 - ETA: 1s - loss: 0.5073 - accuracy: 0.69 - ETA: 1s - loss: 0.5072 - accuracy: 0.69 - ETA: 1s - loss: 0.5069 - accuracy: 0.70 - ETA: 1s - loss: 0.5061 - accuracy: 0.70 - ETA: 1s - loss: 0.5047 - accuracy: 0.70 - ETA: 1s - loss: 0.5035 - accuracy: 0.70 - ETA: 1s - loss: 0.5020 - accuracy: 0.70 - ETA: 1s - loss: 0.5015 - accuracy: 0.70 - ETA: 1s - loss: 0.5005 - accuracy: 0.70 - ETA: 1s - loss: 0.4998 - accuracy: 0.70 - ETA: 1s - loss: 0.4990 - accuracy: 0.70 - ETA: 0s - loss: 0.4981 - accuracy: 0.70 - ETA: 0s - loss: 0.4980 - accuracy: 0.70 - ETA: 0s - loss: 0.4973 - accuracy: 0.70 - ETA: 0s - loss: 0.4963 - accuracy: 0.71 - ETA: 0s - loss: 0.4953 - accuracy: 0.71 - ETA: 0s - loss: 0.4942 - accuracy: 0.71 - ETA: 0s - loss: 0.4935 - accuracy: 0.71 - ETA: 0s - loss: 0.4933 - accuracy: 0.71 - ETA: 0s - loss: 0.4923 - accuracy: 0.71 - ETA: 0s - loss: 0.4909 - accuracy: 0.71 - ETA: 0s - loss: 0.4899 - accuracy: 0.71 - ETA: 0s - loss: 0.4891 - accuracy: 0.71 - ETA: 0s - loss: 0.4881 - accuracy: 0.71 - ETA: 0s - loss: 0.4874 - accuracy: 0.71 - ETA: 0s - loss: 0.4869 - accuracy: 0.71 - ETA: 0s - loss: 0.4860 - accuracy: 0.72 - ETA: 0s - loss: 0.4853 - accuracy: 0.72 - 8s 329us/sample - loss: 0.4851 - accuracy: 0.7210\n",
      "Epoch 2/5\n",
      "25000/25000 [==============================] - ETA: 9s - loss: 0.4066 - accuracy: 0.81 - ETA: 7s - loss: 0.3465 - accuracy: 0.86 - ETA: 7s - loss: 0.3233 - accuracy: 0.86 - ETA: 7s - loss: 0.3070 - accuracy: 0.87 - ETA: 7s - loss: 0.2990 - accuracy: 0.87 - ETA: 7s - loss: 0.3059 - accuracy: 0.86 - ETA: 6s - loss: 0.2947 - accuracy: 0.87 - ETA: 7s - loss: 0.2913 - accuracy: 0.87 - ETA: 6s - loss: 0.2912 - accuracy: 0.87 - ETA: 6s - loss: 0.2879 - accuracy: 0.87 - ETA: 6s - loss: 0.2912 - accuracy: 0.87 - ETA: 6s - loss: 0.2942 - accuracy: 0.87 - ETA: 6s - loss: 0.2959 - accuracy: 0.87 - ETA: 6s - loss: 0.3037 - accuracy: 0.86 - ETA: 7s - loss: 0.3032 - accuracy: 0.86 - ETA: 6s - loss: 0.3015 - accuracy: 0.86 - ETA: 6s - loss: 0.3019 - accuracy: 0.86 - ETA: 6s - loss: 0.3014 - accuracy: 0.86 - ETA: 6s - loss: 0.3018 - accuracy: 0.86 - ETA: 6s - loss: 0.2991 - accuracy: 0.86 - ETA: 6s - loss: 0.2972 - accuracy: 0.86 - ETA: 6s - loss: 0.2961 - accuracy: 0.86 - ETA: 6s - loss: 0.2955 - accuracy: 0.86 - ETA: 6s - loss: 0.2950 - accuracy: 0.86 - ETA: 6s - loss: 0.2946 - accuracy: 0.86 - ETA: 6s - loss: 0.2930 - accuracy: 0.86 - ETA: 6s - loss: 0.2931 - accuracy: 0.86 - ETA: 6s - loss: 0.2922 - accuracy: 0.86 - ETA: 6s - loss: 0.2936 - accuracy: 0.86 - ETA: 6s - loss: 0.2950 - accuracy: 0.86 - ETA: 6s - loss: 0.2951 - accuracy: 0.86 - ETA: 5s - loss: 0.2950 - accuracy: 0.86 - ETA: 5s - loss: 0.2962 - accuracy: 0.86 - ETA: 5s - loss: 0.2974 - accuracy: 0.86 - ETA: 5s - loss: 0.2976 - accuracy: 0.86 - ETA: 5s - loss: 0.2967 - accuracy: 0.86 - ETA: 5s - loss: 0.2976 - accuracy: 0.86 - ETA: 5s - loss: 0.2980 - accuracy: 0.86 - ETA: 5s - loss: 0.2974 - accuracy: 0.86 - ETA: 5s - loss: 0.2976 - accuracy: 0.86 - ETA: 5s - loss: 0.2991 - accuracy: 0.86 - ETA: 5s - loss: 0.2984 - accuracy: 0.86 - ETA: 5s - loss: 0.2984 - accuracy: 0.86 - ETA: 5s - loss: 0.2972 - accuracy: 0.86 - ETA: 5s - loss: 0.2968 - accuracy: 0.86 - ETA: 5s - loss: 0.2960 - accuracy: 0.86 - ETA: 5s - loss: 0.2969 - accuracy: 0.86 - ETA: 5s - loss: 0.2964 - accuracy: 0.86 - ETA: 5s - loss: 0.2982 - accuracy: 0.86 - ETA: 5s - loss: 0.2998 - accuracy: 0.86 - ETA: 4s - loss: 0.3000 - accuracy: 0.86 - ETA: 4s - loss: 0.3001 - accuracy: 0.86 - ETA: 4s - loss: 0.2997 - accuracy: 0.86 - ETA: 4s - loss: 0.3004 - accuracy: 0.86 - ETA: 4s - loss: 0.3015 - accuracy: 0.86 - ETA: 4s - loss: 0.3024 - accuracy: 0.86 - ETA: 4s - loss: 0.3014 - accuracy: 0.86 - ETA: 4s - loss: 0.3023 - accuracy: 0.86 - ETA: 4s - loss: 0.3021 - accuracy: 0.86 - ETA: 4s - loss: 0.3029 - accuracy: 0.86 - ETA: 4s - loss: 0.3053 - accuracy: 0.86 - ETA: 4s - loss: 0.3058 - accuracy: 0.86 - ETA: 4s - loss: 0.3057 - accuracy: 0.86 - ETA: 4s - loss: 0.3047 - accuracy: 0.86 - ETA: 4s - loss: 0.3052 - accuracy: 0.86 - ETA: 4s - loss: 0.3051 - accuracy: 0.86 - ETA: 3s - loss: 0.3051 - accuracy: 0.86 - ETA: 3s - loss: 0.3045 - accuracy: 0.86 - ETA: 3s - loss: 0.3046 - accuracy: 0.86 - ETA: 3s - loss: 0.3049 - accuracy: 0.86 - ETA: 3s - loss: 0.3052 - accuracy: 0.86 - ETA: 3s - loss: 0.3049 - accuracy: 0.86 - ETA: 3s - loss: 0.3061 - accuracy: 0.86 - ETA: 3s - loss: 0.3059 - accuracy: 0.86 - ETA: 3s - loss: 0.3063 - accuracy: 0.86 - ETA: 3s - loss: 0.3060 - accuracy: 0.86 - ETA: 3s - loss: 0.3068 - accuracy: 0.86 - ETA: 3s - loss: 0.3065 - accuracy: 0.86 - ETA: 3s - loss: 0.3064 - accuracy: 0.86 - ETA: 3s - loss: 0.3065 - accuracy: 0.86 - ETA: 3s - loss: 0.3070 - accuracy: 0.86 - ETA: 3s - loss: 0.3066 - accuracy: 0.86 - ETA: 3s - loss: 0.3058 - accuracy: 0.86 - ETA: 3s - loss: 0.3052 - accuracy: 0.86 - ETA: 2s - loss: 0.3053 - accuracy: 0.86 - ETA: 2s - loss: 0.3048 - accuracy: 0.86 - ETA: 2s - loss: 0.3054 - accuracy: 0.86 - ETA: 2s - loss: 0.3057 - accuracy: 0.86 - ETA: 2s - loss: 0.3052 - accuracy: 0.86 - ETA: 2s - loss: 0.3054 - accuracy: 0.86 - ETA: 2s - loss: 0.3050 - accuracy: 0.86 - ETA: 2s - loss: 0.3048 - accuracy: 0.86 - ETA: 2s - loss: 0.3054 - accuracy: 0.86 - ETA: 2s - loss: 0.3058 - accuracy: 0.86 - ETA: 2s - loss: 0.3055 - accuracy: 0.86 - ETA: 2s - loss: 0.3060 - accuracy: 0.86 - ETA: 2s - loss: 0.3053 - accuracy: 0.86 - ETA: 2s - loss: 0.3054 - accuracy: 0.86 - ETA: 2s - loss: 0.3060 - accuracy: 0.86 - ETA: 2s - loss: 0.3063 - accuracy: 0.86 - ETA: 2s - loss: 0.3064 - accuracy: 0.86 - ETA: 1s - loss: 0.3060 - accuracy: 0.86 - ETA: 1s - loss: 0.3061 - accuracy: 0.86 - ETA: 1s - loss: 0.3058 - accuracy: 0.86 - ETA: 1s - loss: 0.3054 - accuracy: 0.86 - ETA: 1s - loss: 0.3048 - accuracy: 0.86 - ETA: 1s - loss: 0.3046 - accuracy: 0.86 - ETA: 1s - loss: 0.3043 - accuracy: 0.86 - ETA: 1s - loss: 0.3040 - accuracy: 0.86 - ETA: 1s - loss: 0.3038 - accuracy: 0.86 - ETA: 1s - loss: 0.3038 - accuracy: 0.86 - ETA: 1s - loss: 0.3040 - accuracy: 0.86 - ETA: 1s - loss: 0.3038 - accuracy: 0.86 - ETA: 1s - loss: 0.3040 - accuracy: 0.86 - ETA: 1s - loss: 0.3049 - accuracy: 0.86 - ETA: 1s - loss: 0.3046 - accuracy: 0.86 - ETA: 1s - loss: 0.3050 - accuracy: 0.86 - ETA: 1s - loss: 0.3052 - accuracy: 0.86 - ETA: 0s - loss: 0.3059 - accuracy: 0.86 - ETA: 0s - loss: 0.3059 - accuracy: 0.86 - ETA: 0s - loss: 0.3059 - accuracy: 0.86 - ETA: 0s - loss: 0.3062 - accuracy: 0.86 - ETA: 0s - loss: 0.3063 - accuracy: 0.86 - ETA: 0s - loss: 0.3069 - accuracy: 0.86 - ETA: 0s - loss: 0.3071 - accuracy: 0.86 - ETA: 0s - loss: 0.3067 - accuracy: 0.86 - ETA: 0s - loss: 0.3063 - accuracy: 0.86 - ETA: 0s - loss: 0.3062 - accuracy: 0.86 - ETA: 0s - loss: 0.3067 - accuracy: 0.86 - ETA: 0s - loss: 0.3065 - accuracy: 0.86 - ETA: 0s - loss: 0.3063 - accuracy: 0.86 - ETA: 0s - loss: 0.3060 - accuracy: 0.86 - ETA: 0s - loss: 0.3067 - accuracy: 0.86 - ETA: 0s - loss: 0.3067 - accuracy: 0.86 - ETA: 0s - loss: 0.3066 - accuracy: 0.86 - ETA: 0s - loss: 0.3066 - accuracy: 0.86 - 8s 304us/sample - loss: 0.3066 - accuracy: 0.8614\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - ETA: 8s - loss: 0.2434 - accuracy: 0.87 - ETA: 7s - loss: 0.2472 - accuracy: 0.87 - ETA: 7s - loss: 0.2343 - accuracy: 0.89 - ETA: 7s - loss: 0.2420 - accuracy: 0.89 - ETA: 7s - loss: 0.2381 - accuracy: 0.90 - ETA: 7s - loss: 0.2390 - accuracy: 0.89 - ETA: 7s - loss: 0.2352 - accuracy: 0.90 - ETA: 7s - loss: 0.2383 - accuracy: 0.90 - ETA: 7s - loss: 0.2415 - accuracy: 0.90 - ETA: 6s - loss: 0.2442 - accuracy: 0.90 - ETA: 6s - loss: 0.2456 - accuracy: 0.89 - ETA: 6s - loss: 0.2427 - accuracy: 0.90 - ETA: 6s - loss: 0.2437 - accuracy: 0.90 - ETA: 6s - loss: 0.2429 - accuracy: 0.89 - ETA: 6s - loss: 0.2488 - accuracy: 0.89 - ETA: 6s - loss: 0.2463 - accuracy: 0.89 - ETA: 6s - loss: 0.2441 - accuracy: 0.89 - ETA: 6s - loss: 0.2438 - accuracy: 0.89 - ETA: 6s - loss: 0.2465 - accuracy: 0.89 - ETA: 6s - loss: 0.2473 - accuracy: 0.89 - ETA: 6s - loss: 0.2465 - accuracy: 0.90 - ETA: 6s - loss: 0.2464 - accuracy: 0.89 - ETA: 6s - loss: 0.2444 - accuracy: 0.90 - ETA: 6s - loss: 0.2420 - accuracy: 0.90 - ETA: 6s - loss: 0.2427 - accuracy: 0.90 - ETA: 6s - loss: 0.2398 - accuracy: 0.90 - ETA: 6s - loss: 0.2389 - accuracy: 0.90 - ETA: 6s - loss: 0.2399 - accuracy: 0.90 - ETA: 6s - loss: 0.2382 - accuracy: 0.90 - ETA: 5s - loss: 0.2390 - accuracy: 0.90 - ETA: 5s - loss: 0.2381 - accuracy: 0.90 - ETA: 5s - loss: 0.2386 - accuracy: 0.90 - ETA: 5s - loss: 0.2388 - accuracy: 0.90 - ETA: 5s - loss: 0.2378 - accuracy: 0.90 - ETA: 5s - loss: 0.2372 - accuracy: 0.90 - ETA: 5s - loss: 0.2377 - accuracy: 0.90 - ETA: 5s - loss: 0.2366 - accuracy: 0.90 - ETA: 5s - loss: 0.2372 - accuracy: 0.90 - ETA: 5s - loss: 0.2363 - accuracy: 0.90 - ETA: 5s - loss: 0.2353 - accuracy: 0.90 - ETA: 5s - loss: 0.2356 - accuracy: 0.90 - ETA: 5s - loss: 0.2356 - accuracy: 0.90 - ETA: 5s - loss: 0.2353 - accuracy: 0.90 - ETA: 5s - loss: 0.2366 - accuracy: 0.90 - ETA: 5s - loss: 0.2381 - accuracy: 0.90 - ETA: 5s - loss: 0.2380 - accuracy: 0.90 - ETA: 5s - loss: 0.2390 - accuracy: 0.90 - ETA: 5s - loss: 0.2397 - accuracy: 0.90 - ETA: 5s - loss: 0.2392 - accuracy: 0.89 - ETA: 5s - loss: 0.2398 - accuracy: 0.89 - ETA: 5s - loss: 0.2396 - accuracy: 0.89 - ETA: 5s - loss: 0.2398 - accuracy: 0.89 - ETA: 4s - loss: 0.2396 - accuracy: 0.89 - ETA: 4s - loss: 0.2397 - accuracy: 0.89 - ETA: 4s - loss: 0.2408 - accuracy: 0.89 - ETA: 4s - loss: 0.2408 - accuracy: 0.89 - ETA: 4s - loss: 0.2402 - accuracy: 0.89 - ETA: 4s - loss: 0.2403 - accuracy: 0.89 - ETA: 4s - loss: 0.2400 - accuracy: 0.89 - ETA: 4s - loss: 0.2389 - accuracy: 0.89 - ETA: 4s - loss: 0.2400 - accuracy: 0.89 - ETA: 4s - loss: 0.2393 - accuracy: 0.89 - ETA: 4s - loss: 0.2401 - accuracy: 0.89 - ETA: 4s - loss: 0.2408 - accuracy: 0.89 - ETA: 4s - loss: 0.2404 - accuracy: 0.89 - ETA: 4s - loss: 0.2417 - accuracy: 0.89 - ETA: 4s - loss: 0.2418 - accuracy: 0.89 - ETA: 4s - loss: 0.2424 - accuracy: 0.89 - ETA: 4s - loss: 0.2426 - accuracy: 0.89 - ETA: 4s - loss: 0.2425 - accuracy: 0.89 - ETA: 3s - loss: 0.2426 - accuracy: 0.89 - ETA: 3s - loss: 0.2438 - accuracy: 0.89 - ETA: 3s - loss: 0.2453 - accuracy: 0.89 - ETA: 3s - loss: 0.2449 - accuracy: 0.89 - ETA: 3s - loss: 0.2450 - accuracy: 0.89 - ETA: 3s - loss: 0.2454 - accuracy: 0.89 - ETA: 3s - loss: 0.2456 - accuracy: 0.89 - ETA: 3s - loss: 0.2451 - accuracy: 0.89 - ETA: 3s - loss: 0.2452 - accuracy: 0.89 - ETA: 3s - loss: 0.2456 - accuracy: 0.89 - ETA: 3s - loss: 0.2463 - accuracy: 0.89 - ETA: 3s - loss: 0.2470 - accuracy: 0.89 - ETA: 3s - loss: 0.2466 - accuracy: 0.89 - ETA: 3s - loss: 0.2464 - accuracy: 0.89 - ETA: 3s - loss: 0.2473 - accuracy: 0.89 - ETA: 3s - loss: 0.2470 - accuracy: 0.89 - ETA: 3s - loss: 0.2471 - accuracy: 0.89 - ETA: 2s - loss: 0.2473 - accuracy: 0.89 - ETA: 2s - loss: 0.2476 - accuracy: 0.89 - ETA: 2s - loss: 0.2485 - accuracy: 0.89 - ETA: 2s - loss: 0.2495 - accuracy: 0.89 - ETA: 2s - loss: 0.2496 - accuracy: 0.89 - ETA: 2s - loss: 0.2506 - accuracy: 0.89 - ETA: 2s - loss: 0.2514 - accuracy: 0.89 - ETA: 2s - loss: 0.2525 - accuracy: 0.89 - ETA: 2s - loss: 0.2524 - accuracy: 0.89 - ETA: 2s - loss: 0.2531 - accuracy: 0.89 - ETA: 2s - loss: 0.2533 - accuracy: 0.89 - ETA: 2s - loss: 0.2538 - accuracy: 0.89 - ETA: 2s - loss: 0.2538 - accuracy: 0.89 - ETA: 2s - loss: 0.2541 - accuracy: 0.89 - ETA: 2s - loss: 0.2545 - accuracy: 0.89 - ETA: 2s - loss: 0.2543 - accuracy: 0.89 - ETA: 2s - loss: 0.2548 - accuracy: 0.89 - ETA: 2s - loss: 0.2549 - accuracy: 0.89 - ETA: 2s - loss: 0.2552 - accuracy: 0.89 - ETA: 1s - loss: 0.2555 - accuracy: 0.89 - ETA: 1s - loss: 0.2565 - accuracy: 0.89 - ETA: 1s - loss: 0.2575 - accuracy: 0.89 - ETA: 1s - loss: 0.2573 - accuracy: 0.89 - ETA: 1s - loss: 0.2573 - accuracy: 0.89 - ETA: 1s - loss: 0.2572 - accuracy: 0.89 - ETA: 1s - loss: 0.2569 - accuracy: 0.89 - ETA: 1s - loss: 0.2571 - accuracy: 0.89 - ETA: 1s - loss: 0.2567 - accuracy: 0.89 - ETA: 1s - loss: 0.2576 - accuracy: 0.89 - ETA: 1s - loss: 0.2582 - accuracy: 0.89 - ETA: 1s - loss: 0.2584 - accuracy: 0.89 - ETA: 1s - loss: 0.2579 - accuracy: 0.89 - ETA: 1s - loss: 0.2582 - accuracy: 0.89 - ETA: 1s - loss: 0.2588 - accuracy: 0.88 - ETA: 1s - loss: 0.2592 - accuracy: 0.88 - ETA: 1s - loss: 0.2596 - accuracy: 0.88 - ETA: 1s - loss: 0.2594 - accuracy: 0.88 - ETA: 1s - loss: 0.2603 - accuracy: 0.88 - ETA: 0s - loss: 0.2605 - accuracy: 0.88 - ETA: 0s - loss: 0.2602 - accuracy: 0.88 - ETA: 0s - loss: 0.2605 - accuracy: 0.88 - ETA: 0s - loss: 0.2611 - accuracy: 0.88 - ETA: 0s - loss: 0.2611 - accuracy: 0.88 - ETA: 0s - loss: 0.2609 - accuracy: 0.88 - ETA: 0s - loss: 0.2616 - accuracy: 0.88 - ETA: 0s - loss: 0.2615 - accuracy: 0.88 - ETA: 0s - loss: 0.2613 - accuracy: 0.88 - ETA: 0s - loss: 0.2612 - accuracy: 0.88 - ETA: 0s - loss: 0.2614 - accuracy: 0.88 - ETA: 0s - loss: 0.2614 - accuracy: 0.88 - ETA: 0s - loss: 0.2615 - accuracy: 0.88 - ETA: 0s - loss: 0.2619 - accuracy: 0.88 - ETA: 0s - loss: 0.2619 - accuracy: 0.88 - ETA: 0s - loss: 0.2621 - accuracy: 0.88 - ETA: 0s - loss: 0.2619 - accuracy: 0.88 - ETA: 0s - loss: 0.2616 - accuracy: 0.88 - 8s 318us/sample - loss: 0.2619 - accuracy: 0.8872\n",
      "Epoch 4/5\n",
      "25000/25000 [==============================] - ETA: 8s - loss: 0.1460 - accuracy: 0.93 - ETA: 7s - loss: 0.1867 - accuracy: 0.93 - ETA: 8s - loss: 0.1888 - accuracy: 0.92 - ETA: 7s - loss: 0.1793 - accuracy: 0.93 - ETA: 8s - loss: 0.1819 - accuracy: 0.93 - ETA: 8s - loss: 0.1764 - accuracy: 0.93 - ETA: 7s - loss: 0.1878 - accuracy: 0.93 - ETA: 7s - loss: 0.1900 - accuracy: 0.93 - ETA: 7s - loss: 0.1860 - accuracy: 0.93 - ETA: 7s - loss: 0.1883 - accuracy: 0.93 - ETA: 7s - loss: 0.1983 - accuracy: 0.92 - ETA: 7s - loss: 0.1982 - accuracy: 0.92 - ETA: 7s - loss: 0.2022 - accuracy: 0.92 - ETA: 7s - loss: 0.2028 - accuracy: 0.92 - ETA: 7s - loss: 0.2010 - accuracy: 0.92 - ETA: 7s - loss: 0.2016 - accuracy: 0.92 - ETA: 7s - loss: 0.2045 - accuracy: 0.92 - ETA: 6s - loss: 0.2033 - accuracy: 0.92 - ETA: 6s - loss: 0.2014 - accuracy: 0.92 - ETA: 6s - loss: 0.2044 - accuracy: 0.92 - ETA: 6s - loss: 0.2044 - accuracy: 0.92 - ETA: 6s - loss: 0.2016 - accuracy: 0.92 - ETA: 6s - loss: 0.2015 - accuracy: 0.92 - ETA: 6s - loss: 0.2017 - accuracy: 0.92 - ETA: 6s - loss: 0.2041 - accuracy: 0.92 - ETA: 6s - loss: 0.2036 - accuracy: 0.92 - ETA: 6s - loss: 0.2044 - accuracy: 0.92 - ETA: 6s - loss: 0.2055 - accuracy: 0.92 - ETA: 6s - loss: 0.2063 - accuracy: 0.92 - ETA: 6s - loss: 0.2061 - accuracy: 0.92 - ETA: 6s - loss: 0.2072 - accuracy: 0.92 - ETA: 6s - loss: 0.2066 - accuracy: 0.92 - ETA: 6s - loss: 0.2088 - accuracy: 0.92 - ETA: 5s - loss: 0.2090 - accuracy: 0.92 - ETA: 5s - loss: 0.2081 - accuracy: 0.92 - ETA: 5s - loss: 0.2066 - accuracy: 0.92 - ETA: 5s - loss: 0.2066 - accuracy: 0.92 - ETA: 5s - loss: 0.2070 - accuracy: 0.92 - ETA: 5s - loss: 0.2070 - accuracy: 0.92 - ETA: 5s - loss: 0.2079 - accuracy: 0.92 - ETA: 5s - loss: 0.2082 - accuracy: 0.92 - ETA: 5s - loss: 0.2085 - accuracy: 0.92 - ETA: 5s - loss: 0.2070 - accuracy: 0.92 - ETA: 5s - loss: 0.2063 - accuracy: 0.92 - ETA: 5s - loss: 0.2059 - accuracy: 0.92 - ETA: 5s - loss: 0.2062 - accuracy: 0.92 - ETA: 5s - loss: 0.2059 - accuracy: 0.92 - ETA: 5s - loss: 0.2052 - accuracy: 0.92 - ETA: 5s - loss: 0.2060 - accuracy: 0.92 - ETA: 5s - loss: 0.2058 - accuracy: 0.92 - ETA: 5s - loss: 0.2054 - accuracy: 0.92 - ETA: 4s - loss: 0.2065 - accuracy: 0.92 - ETA: 4s - loss: 0.2066 - accuracy: 0.92 - ETA: 4s - loss: 0.2079 - accuracy: 0.91 - ETA: 4s - loss: 0.2085 - accuracy: 0.91 - ETA: 4s - loss: 0.2092 - accuracy: 0.91 - ETA: 4s - loss: 0.2093 - accuracy: 0.91 - ETA: 4s - loss: 0.2105 - accuracy: 0.91 - ETA: 4s - loss: 0.2104 - accuracy: 0.91 - ETA: 4s - loss: 0.2113 - accuracy: 0.91 - ETA: 4s - loss: 0.2112 - accuracy: 0.91 - ETA: 4s - loss: 0.2141 - accuracy: 0.91 - ETA: 4s - loss: 0.2133 - accuracy: 0.91 - ETA: 4s - loss: 0.2136 - accuracy: 0.91 - ETA: 4s - loss: 0.2142 - accuracy: 0.91 - ETA: 4s - loss: 0.2156 - accuracy: 0.91 - ETA: 4s - loss: 0.2163 - accuracy: 0.91 - ETA: 4s - loss: 0.2170 - accuracy: 0.91 - ETA: 4s - loss: 0.2160 - accuracy: 0.91 - ETA: 4s - loss: 0.2155 - accuracy: 0.91 - ETA: 3s - loss: 0.2158 - accuracy: 0.91 - ETA: 3s - loss: 0.2156 - accuracy: 0.91 - ETA: 3s - loss: 0.2159 - accuracy: 0.91 - ETA: 3s - loss: 0.2156 - accuracy: 0.91 - ETA: 3s - loss: 0.2153 - accuracy: 0.91 - ETA: 3s - loss: 0.2143 - accuracy: 0.91 - ETA: 3s - loss: 0.2142 - accuracy: 0.91 - ETA: 3s - loss: 0.2146 - accuracy: 0.91 - ETA: 3s - loss: 0.2149 - accuracy: 0.91 - ETA: 3s - loss: 0.2149 - accuracy: 0.91 - ETA: 3s - loss: 0.2151 - accuracy: 0.91 - ETA: 3s - loss: 0.2158 - accuracy: 0.91 - ETA: 3s - loss: 0.2157 - accuracy: 0.91 - ETA: 3s - loss: 0.2158 - accuracy: 0.91 - ETA: 3s - loss: 0.2163 - accuracy: 0.91 - ETA: 3s - loss: 0.2158 - accuracy: 0.91 - ETA: 3s - loss: 0.2155 - accuracy: 0.91 - ETA: 2s - loss: 0.2164 - accuracy: 0.91 - ETA: 2s - loss: 0.2162 - accuracy: 0.91 - ETA: 2s - loss: 0.2169 - accuracy: 0.91 - ETA: 2s - loss: 0.2172 - accuracy: 0.91 - ETA: 2s - loss: 0.2173 - accuracy: 0.91 - ETA: 2s - loss: 0.2176 - accuracy: 0.91 - ETA: 2s - loss: 0.2180 - accuracy: 0.91 - ETA: 2s - loss: 0.2190 - accuracy: 0.91 - ETA: 2s - loss: 0.2199 - accuracy: 0.91 - ETA: 2s - loss: 0.2215 - accuracy: 0.91 - ETA: 2s - loss: 0.2215 - accuracy: 0.90 - ETA: 2s - loss: 0.2219 - accuracy: 0.90 - ETA: 2s - loss: 0.2228 - accuracy: 0.90 - ETA: 2s - loss: 0.2236 - accuracy: 0.90 - ETA: 2s - loss: 0.2232 - accuracy: 0.90 - ETA: 2s - loss: 0.2231 - accuracy: 0.90 - ETA: 2s - loss: 0.2242 - accuracy: 0.90 - ETA: 2s - loss: 0.2256 - accuracy: 0.90 - ETA: 1s - loss: 0.2255 - accuracy: 0.90 - ETA: 1s - loss: 0.2253 - accuracy: 0.90 - ETA: 1s - loss: 0.2256 - accuracy: 0.90 - ETA: 1s - loss: 0.2257 - accuracy: 0.90 - ETA: 1s - loss: 0.2256 - accuracy: 0.90 - ETA: 1s - loss: 0.2256 - accuracy: 0.90 - ETA: 1s - loss: 0.2268 - accuracy: 0.90 - ETA: 1s - loss: 0.2275 - accuracy: 0.90 - ETA: 1s - loss: 0.2276 - accuracy: 0.90 - ETA: 1s - loss: 0.2279 - accuracy: 0.90 - ETA: 1s - loss: 0.2273 - accuracy: 0.90 - ETA: 1s - loss: 0.2280 - accuracy: 0.90 - ETA: 1s - loss: 0.2286 - accuracy: 0.90 - ETA: 1s - loss: 0.2292 - accuracy: 0.90 - ETA: 1s - loss: 0.2297 - accuracy: 0.90 - ETA: 1s - loss: 0.2297 - accuracy: 0.90 - ETA: 1s - loss: 0.2296 - accuracy: 0.90 - ETA: 1s - loss: 0.2305 - accuracy: 0.90 - ETA: 0s - loss: 0.2312 - accuracy: 0.90 - ETA: 0s - loss: 0.2316 - accuracy: 0.90 - ETA: 0s - loss: 0.2312 - accuracy: 0.90 - ETA: 0s - loss: 0.2313 - accuracy: 0.90 - ETA: 0s - loss: 0.2314 - accuracy: 0.90 - ETA: 0s - loss: 0.2323 - accuracy: 0.90 - ETA: 0s - loss: 0.2324 - accuracy: 0.90 - ETA: 0s - loss: 0.2328 - accuracy: 0.90 - ETA: 0s - loss: 0.2329 - accuracy: 0.90 - ETA: 0s - loss: 0.2337 - accuracy: 0.90 - ETA: 0s - loss: 0.2340 - accuracy: 0.90 - ETA: 0s - loss: 0.2344 - accuracy: 0.90 - ETA: 0s - loss: 0.2348 - accuracy: 0.90 - ETA: 0s - loss: 0.2351 - accuracy: 0.90 - ETA: 0s - loss: 0.2353 - accuracy: 0.90 - ETA: 0s - loss: 0.2351 - accuracy: 0.90 - ETA: 0s - loss: 0.2354 - accuracy: 0.90 - ETA: 0s - loss: 0.2363 - accuracy: 0.90 - 8s 314us/sample - loss: 0.2367 - accuracy: 0.9008\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - ETA: 8s - loss: 0.1100 - accuracy: 1.00 - ETA: 8s - loss: 0.0963 - accuracy: 0.98 - ETA: 8s - loss: 0.1418 - accuracy: 0.96 - ETA: 8s - loss: 0.1488 - accuracy: 0.95 - ETA: 8s - loss: 0.1560 - accuracy: 0.95 - ETA: 7s - loss: 0.1573 - accuracy: 0.94 - ETA: 7s - loss: 0.1680 - accuracy: 0.94 - ETA: 7s - loss: 0.1730 - accuracy: 0.94 - ETA: 7s - loss: 0.1690 - accuracy: 0.94 - ETA: 7s - loss: 0.1729 - accuracy: 0.93 - ETA: 7s - loss: 0.1721 - accuracy: 0.93 - ETA: 7s - loss: 0.1726 - accuracy: 0.93 - ETA: 7s - loss: 0.1757 - accuracy: 0.93 - ETA: 7s - loss: 0.1739 - accuracy: 0.93 - ETA: 7s - loss: 0.1777 - accuracy: 0.93 - ETA: 7s - loss: 0.1788 - accuracy: 0.93 - ETA: 7s - loss: 0.1795 - accuracy: 0.93 - ETA: 7s - loss: 0.1813 - accuracy: 0.93 - ETA: 7s - loss: 0.1834 - accuracy: 0.93 - ETA: 7s - loss: 0.1811 - accuracy: 0.93 - ETA: 7s - loss: 0.1811 - accuracy: 0.93 - ETA: 6s - loss: 0.1809 - accuracy: 0.93 - ETA: 6s - loss: 0.1814 - accuracy: 0.92 - ETA: 6s - loss: 0.1787 - accuracy: 0.93 - ETA: 6s - loss: 0.1777 - accuracy: 0.93 - ETA: 6s - loss: 0.1784 - accuracy: 0.93 - ETA: 6s - loss: 0.1770 - accuracy: 0.93 - ETA: 6s - loss: 0.1759 - accuracy: 0.93 - ETA: 6s - loss: 0.1772 - accuracy: 0.93 - ETA: 6s - loss: 0.1762 - accuracy: 0.93 - ETA: 6s - loss: 0.1776 - accuracy: 0.93 - ETA: 6s - loss: 0.1777 - accuracy: 0.93 - ETA: 6s - loss: 0.1771 - accuracy: 0.93 - ETA: 6s - loss: 0.1758 - accuracy: 0.93 - ETA: 6s - loss: 0.1752 - accuracy: 0.93 - ETA: 6s - loss: 0.1768 - accuracy: 0.93 - ETA: 6s - loss: 0.1786 - accuracy: 0.92 - ETA: 6s - loss: 0.1790 - accuracy: 0.92 - ETA: 6s - loss: 0.1801 - accuracy: 0.92 - ETA: 6s - loss: 0.1812 - accuracy: 0.92 - ETA: 5s - loss: 0.1821 - accuracy: 0.92 - ETA: 6s - loss: 0.1826 - accuracy: 0.92 - ETA: 5s - loss: 0.1815 - accuracy: 0.92 - ETA: 5s - loss: 0.1810 - accuracy: 0.92 - ETA: 5s - loss: 0.1803 - accuracy: 0.92 - ETA: 5s - loss: 0.1811 - accuracy: 0.92 - ETA: 5s - loss: 0.1808 - accuracy: 0.92 - ETA: 5s - loss: 0.1799 - accuracy: 0.92 - ETA: 5s - loss: 0.1797 - accuracy: 0.92 - ETA: 5s - loss: 0.1800 - accuracy: 0.92 - ETA: 5s - loss: 0.1795 - accuracy: 0.92 - ETA: 5s - loss: 0.1795 - accuracy: 0.92 - ETA: 5s - loss: 0.1801 - accuracy: 0.92 - ETA: 5s - loss: 0.1810 - accuracy: 0.92 - ETA: 5s - loss: 0.1810 - accuracy: 0.92 - ETA: 5s - loss: 0.1806 - accuracy: 0.92 - ETA: 5s - loss: 0.1810 - accuracy: 0.92 - ETA: 5s - loss: 0.1807 - accuracy: 0.92 - ETA: 5s - loss: 0.1807 - accuracy: 0.92 - ETA: 5s - loss: 0.1806 - accuracy: 0.92 - ETA: 4s - loss: 0.1821 - accuracy: 0.92 - ETA: 4s - loss: 0.1828 - accuracy: 0.92 - ETA: 4s - loss: 0.1826 - accuracy: 0.92 - ETA: 4s - loss: 0.1829 - accuracy: 0.92 - ETA: 4s - loss: 0.1829 - accuracy: 0.92 - ETA: 4s - loss: 0.1842 - accuracy: 0.92 - ETA: 4s - loss: 0.1843 - accuracy: 0.92 - ETA: 4s - loss: 0.1847 - accuracy: 0.92 - ETA: 4s - loss: 0.1859 - accuracy: 0.92 - ETA: 4s - loss: 0.1861 - accuracy: 0.92 - ETA: 4s - loss: 0.1868 - accuracy: 0.92 - ETA: 4s - loss: 0.1865 - accuracy: 0.92 - ETA: 4s - loss: 0.1871 - accuracy: 0.92 - ETA: 4s - loss: 0.1879 - accuracy: 0.92 - ETA: 4s - loss: 0.1882 - accuracy: 0.92 - ETA: 4s - loss: 0.1881 - accuracy: 0.92 - ETA: 4s - loss: 0.1879 - accuracy: 0.92 - ETA: 4s - loss: 0.1891 - accuracy: 0.92 - ETA: 4s - loss: 0.1898 - accuracy: 0.92 - ETA: 4s - loss: 0.1900 - accuracy: 0.92 - ETA: 3s - loss: 0.1901 - accuracy: 0.92 - ETA: 3s - loss: 0.1908 - accuracy: 0.92 - ETA: 3s - loss: 0.1913 - accuracy: 0.92 - ETA: 3s - loss: 0.1919 - accuracy: 0.92 - ETA: 3s - loss: 0.1930 - accuracy: 0.91 - ETA: 3s - loss: 0.1932 - accuracy: 0.91 - ETA: 3s - loss: 0.1936 - accuracy: 0.91 - ETA: 3s - loss: 0.1931 - accuracy: 0.91 - ETA: 3s - loss: 0.1946 - accuracy: 0.91 - ETA: 3s - loss: 0.1948 - accuracy: 0.91 - ETA: 3s - loss: 0.1945 - accuracy: 0.91 - ETA: 3s - loss: 0.1945 - accuracy: 0.91 - ETA: 3s - loss: 0.1950 - accuracy: 0.91 - ETA: 3s - loss: 0.1947 - accuracy: 0.91 - ETA: 3s - loss: 0.1949 - accuracy: 0.91 - ETA: 3s - loss: 0.1949 - accuracy: 0.91 - ETA: 3s - loss: 0.1953 - accuracy: 0.91 - ETA: 3s - loss: 0.1956 - accuracy: 0.91 - ETA: 3s - loss: 0.1962 - accuracy: 0.91 - ETA: 2s - loss: 0.1966 - accuracy: 0.91 - ETA: 2s - loss: 0.1970 - accuracy: 0.91 - ETA: 2s - loss: 0.1980 - accuracy: 0.91 - ETA: 2s - loss: 0.1984 - accuracy: 0.91 - ETA: 2s - loss: 0.1993 - accuracy: 0.91 - ETA: 2s - loss: 0.2007 - accuracy: 0.91 - ETA: 2s - loss: 0.2014 - accuracy: 0.91 - ETA: 2s - loss: 0.2019 - accuracy: 0.91 - ETA: 2s - loss: 0.2025 - accuracy: 0.91 - ETA: 2s - loss: 0.2029 - accuracy: 0.91 - ETA: 2s - loss: 0.2031 - accuracy: 0.91 - ETA: 2s - loss: 0.2044 - accuracy: 0.91 - ETA: 2s - loss: 0.2056 - accuracy: 0.91 - ETA: 2s - loss: 0.2061 - accuracy: 0.91 - ETA: 2s - loss: 0.2062 - accuracy: 0.91 - ETA: 2s - loss: 0.2066 - accuracy: 0.91 - ETA: 2s - loss: 0.2065 - accuracy: 0.91 - ETA: 2s - loss: 0.2065 - accuracy: 0.91 - ETA: 2s - loss: 0.2069 - accuracy: 0.91 - ETA: 1s - loss: 0.2079 - accuracy: 0.91 - ETA: 1s - loss: 0.2086 - accuracy: 0.91 - ETA: 1s - loss: 0.2085 - accuracy: 0.91 - ETA: 1s - loss: 0.2085 - accuracy: 0.91 - ETA: 1s - loss: 0.2088 - accuracy: 0.91 - ETA: 1s - loss: 0.2089 - accuracy: 0.91 - ETA: 1s - loss: 0.2089 - accuracy: 0.91 - ETA: 1s - loss: 0.2093 - accuracy: 0.91 - ETA: 1s - loss: 0.2095 - accuracy: 0.91 - ETA: 1s - loss: 0.2099 - accuracy: 0.91 - ETA: 1s - loss: 0.2097 - accuracy: 0.91 - ETA: 1s - loss: 0.2100 - accuracy: 0.91 - ETA: 1s - loss: 0.2105 - accuracy: 0.91 - ETA: 1s - loss: 0.2107 - accuracy: 0.91 - ETA: 1s - loss: 0.2112 - accuracy: 0.91 - ETA: 1s - loss: 0.2121 - accuracy: 0.91 - ETA: 1s - loss: 0.2131 - accuracy: 0.91 - ETA: 1s - loss: 0.2133 - accuracy: 0.91 - ETA: 1s - loss: 0.2142 - accuracy: 0.91 - ETA: 0s - loss: 0.2146 - accuracy: 0.91 - ETA: 0s - loss: 0.2149 - accuracy: 0.91 - ETA: 0s - loss: 0.2150 - accuracy: 0.91 - ETA: 0s - loss: 0.2152 - accuracy: 0.91 - ETA: 0s - loss: 0.2156 - accuracy: 0.90 - ETA: 0s - loss: 0.2155 - accuracy: 0.90 - ETA: 0s - loss: 0.2158 - accuracy: 0.90 - ETA: 0s - loss: 0.2161 - accuracy: 0.90 - ETA: 0s - loss: 0.2166 - accuracy: 0.90 - ETA: 0s - loss: 0.2169 - accuracy: 0.90 - ETA: 0s - loss: 0.2179 - accuracy: 0.90 - ETA: 0s - loss: 0.2177 - accuracy: 0.90 - ETA: 0s - loss: 0.2180 - accuracy: 0.90 - ETA: 0s - loss: 0.2183 - accuracy: 0.90 - ETA: 0s - loss: 0.2187 - accuracy: 0.90 - ETA: 0s - loss: 0.2188 - accuracy: 0.90 - ETA: 0s - loss: 0.2195 - accuracy: 0.90 - ETA: 0s - loss: 0.2198 - accuracy: 0.90 - ETA: 0s - loss: 0.2195 - accuracy: 0.90 - 8s 338us/sample - loss: 0.2194 - accuracy: 0.9074\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# evaluate the trained model on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - ETA: 51s - loss: 0.3425 - accuracy: 0.843 - ETA: 2s - loss: 0.5149 - accuracy: 0.804 - ETA: 1s - loss: 0.5272 - accuracy: 0.79 - ETA: 1s - loss: 0.5317 - accuracy: 0.79 - ETA: 1s - loss: 0.5253 - accuracy: 0.78 - ETA: 0s - loss: 0.5279 - accuracy: 0.78 - ETA: 0s - loss: 0.5234 - accuracy: 0.78 - ETA: 0s - loss: 0.5221 - accuracy: 0.78 - ETA: 0s - loss: 0.5206 - accuracy: 0.79 - ETA: 0s - loss: 0.5203 - accuracy: 0.79 - ETA: 0s - loss: 0.5202 - accuracy: 0.78 - ETA: 0s - loss: 0.5163 - accuracy: 0.79 - ETA: 0s - loss: 0.5109 - accuracy: 0.79 - ETA: 0s - loss: 0.5104 - accuracy: 0.79 - ETA: 0s - loss: 0.5072 - accuracy: 0.79 - ETA: 0s - loss: 0.5071 - accuracy: 0.79 - ETA: 0s - loss: 0.5079 - accuracy: 0.79 - ETA: 0s - loss: 0.5111 - accuracy: 0.79 - ETA: 0s - loss: 0.5096 - accuracy: 0.79 - 1s 40us/sample - loss: 0.5098 - accuracy: 0.7938\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5098185027313232, 0.79376]"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
