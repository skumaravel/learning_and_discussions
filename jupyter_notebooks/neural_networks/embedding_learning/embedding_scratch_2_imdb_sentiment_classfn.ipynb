{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow as tf\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# imdb_reviews/submwords8k - 8000 vocab size\n",
    "# read train_data, test_data and info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "(train_data, test_data), info = tfds.load('imdb_reviews/subwords8k', split=(tfds.Split.TRAIN, tfds.Split.TEST), \n",
    "                                          with_info=True, as_supervised=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Type of 'info'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow_datasets.core.dataset_info.DatasetInfo"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is in 'info'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tfds.core.DatasetInfo(\n",
       "    name='imdb_reviews',\n",
       "    version=1.0.0,\n",
       "    description='Large Movie Review Dataset.\n",
       "This is a dataset for binary sentiment classification containing substantially more data than previous benchmark datasets. We provide a set of 25,000 highly polar movie reviews for training, and 25,000 for testing. There is additional unlabeled data for use as well.',\n",
       "    homepage='http://ai.stanford.edu/~amaas/data/sentiment/',\n",
       "    features=FeaturesDict({\n",
       "        'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=2),\n",
       "        'text': Text(shape=(None,), dtype=tf.int64, encoder=<SubwordTextEncoder vocab_size=8185>),\n",
       "    }),\n",
       "    total_num_examples=100000,\n",
       "    splits={\n",
       "        'test': 25000,\n",
       "        'train': 25000,\n",
       "        'unsupervised': 50000,\n",
       "    },\n",
       "    supervised_keys=('text', 'label'),\n",
       "    citation=\"\"\"@InProceedings{maas-EtAl:2011:ACL-HLT2011,\n",
       "      author    = {Maas, Andrew L.  and  Daly, Raymond E.  and  Pham, Peter T.  and  Huang, Dan  and  Ng, Andrew Y.  and  Potts, Christopher},\n",
       "      title     = {Learning Word Vectors for Sentiment Analysis},\n",
       "      booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},\n",
       "      month     = {June},\n",
       "      year      = {2011},\n",
       "      address   = {Portland, Oregon, USA},\n",
       "      publisher = {Association for Computational Linguistics},\n",
       "      pages     = {142--150},\n",
       "      url       = {http://www.aclweb.org/anthology/P11-1015}\n",
       "    }\"\"\",\n",
       "    redistribution_info=,\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# info.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FeaturesDict({\n",
       "    'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=2),\n",
       "    'text': Text(shape=(None,), dtype=tf.int64, encoder=<SubwordTextEncoder vocab_size=8185>),\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info.features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# info.features['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(shape=(None,), dtype=tf.int64, encoder=<SubwordTextEncoder vocab_size=8185>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info.features['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# info.features['text'].encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<SubwordTextEncoder vocab_size=8185>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info.features['text'].encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = info.features['text'].encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# info.features['text'].encoder.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8185"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# info.features['text'].encoder.subwords - after tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the_',\n",
       " ', ',\n",
       " '. ',\n",
       " 'a_',\n",
       " 'and_',\n",
       " 'of_',\n",
       " 'to_',\n",
       " 's_',\n",
       " 'is_',\n",
       " 'br',\n",
       " 'in_',\n",
       " 'I_',\n",
       " 'that_',\n",
       " 'this_',\n",
       " 'it_',\n",
       " ' /><',\n",
       " ' />',\n",
       " 'was_',\n",
       " 'The_',\n",
       " 'as_',\n",
       " 't_',\n",
       " 'with_',\n",
       " 'for_',\n",
       " '.<',\n",
       " 'on_',\n",
       " 'but_',\n",
       " 'movie_',\n",
       " ' (',\n",
       " 'are_',\n",
       " 'his_',\n",
       " 'have_',\n",
       " 'film_',\n",
       " 'not_',\n",
       " 'ing_',\n",
       " 'be_',\n",
       " 'ed_',\n",
       " 'you_',\n",
       " ' \"',\n",
       " 'it',\n",
       " 'd_',\n",
       " 'an_',\n",
       " 'he_',\n",
       " 'by_',\n",
       " 'at_',\n",
       " 'one_',\n",
       " 'who_',\n",
       " 'y_',\n",
       " 'from_',\n",
       " 'e_',\n",
       " 'or_',\n",
       " 'all_',\n",
       " 'like_',\n",
       " 'they_',\n",
       " '\" ',\n",
       " 'so_',\n",
       " 'just_',\n",
       " 'has_',\n",
       " ') ',\n",
       " 'her_',\n",
       " 'about_',\n",
       " 'out_',\n",
       " 'This_',\n",
       " 'some_',\n",
       " 'ly_',\n",
       " 'movie',\n",
       " 'film',\n",
       " 'very_',\n",
       " 'more_',\n",
       " 'It_',\n",
       " 'would_',\n",
       " 'what_',\n",
       " 'when_',\n",
       " 'which_',\n",
       " 'good_',\n",
       " 'if_',\n",
       " 'up_',\n",
       " 'only_',\n",
       " 'even_',\n",
       " 'their_',\n",
       " 'had_',\n",
       " 'really_',\n",
       " 'my_',\n",
       " 'can_',\n",
       " 'no_',\n",
       " 'were_',\n",
       " 'see_',\n",
       " 'she_',\n",
       " '? ',\n",
       " 'than_',\n",
       " '! ',\n",
       " 'there_',\n",
       " 'get_',\n",
       " 'been_',\n",
       " 'into_',\n",
       " ' - ',\n",
       " 'will_',\n",
       " 'much_',\n",
       " 'story_',\n",
       " 'because_',\n",
       " 'ing',\n",
       " 'time_',\n",
       " 'n_',\n",
       " 'we_',\n",
       " 'ed',\n",
       " 'me_',\n",
       " ': ',\n",
       " 'most_',\n",
       " 'other_',\n",
       " 'don',\n",
       " 'do_',\n",
       " 'm_',\n",
       " 'es_',\n",
       " 'how_',\n",
       " 'also_',\n",
       " 'make_',\n",
       " 'its_',\n",
       " 'could_',\n",
       " 'first_',\n",
       " 'any_',\n",
       " \"' \",\n",
       " 'people_',\n",
       " 'great_',\n",
       " 've_',\n",
       " 'ly',\n",
       " 'er_',\n",
       " 'made_',\n",
       " 'r_',\n",
       " 'But_',\n",
       " 'think_',\n",
       " \" '\",\n",
       " 'i_',\n",
       " 'bad_',\n",
       " 'A_',\n",
       " 'And_',\n",
       " 'It',\n",
       " 'on',\n",
       " '; ',\n",
       " 'him_',\n",
       " 'being_',\n",
       " 'never_',\n",
       " 'way_',\n",
       " 'that',\n",
       " 'many_',\n",
       " 'then_',\n",
       " 'where_',\n",
       " 'two_',\n",
       " 'In_',\n",
       " 'after_',\n",
       " 'too_',\n",
       " 'little_',\n",
       " 'you',\n",
       " '), ',\n",
       " 'well_',\n",
       " 'ng_',\n",
       " 'your_',\n",
       " 'If_',\n",
       " 'l_',\n",
       " '). ',\n",
       " 'does_',\n",
       " 'ever_',\n",
       " 'them_',\n",
       " 'did_',\n",
       " 'watch_',\n",
       " 'know_',\n",
       " 'seen_',\n",
       " 'time',\n",
       " 'er',\n",
       " 'character_',\n",
       " 'over_',\n",
       " 'characters_',\n",
       " 'movies_',\n",
       " 'man_',\n",
       " 'There_',\n",
       " 'love_',\n",
       " 'best_',\n",
       " 'still_',\n",
       " 'off_',\n",
       " 'such_',\n",
       " 'in',\n",
       " 'should_',\n",
       " 'the',\n",
       " 're_',\n",
       " 'He_',\n",
       " 'plot_',\n",
       " 'films_',\n",
       " 'go_',\n",
       " 'these_',\n",
       " 'acting_',\n",
       " 'doesn',\n",
       " 'es',\n",
       " 'show_',\n",
       " 'through_',\n",
       " 'better_',\n",
       " 'al_',\n",
       " 'something_',\n",
       " 'didn',\n",
       " 'back_',\n",
       " 'those_',\n",
       " 'us_',\n",
       " 'less_',\n",
       " '...',\n",
       " 'say_',\n",
       " 'is',\n",
       " 'one',\n",
       " 'makes_',\n",
       " 'and',\n",
       " 'can',\n",
       " 'all',\n",
       " 'ion_',\n",
       " 'find_',\n",
       " 'scene_',\n",
       " 'old_',\n",
       " 'real_',\n",
       " 'few_',\n",
       " 'going_',\n",
       " 'well',\n",
       " 'actually_',\n",
       " 'watching_',\n",
       " 'life_',\n",
       " 'me',\n",
       " '. <',\n",
       " 'o_',\n",
       " 'man',\n",
       " 'there',\n",
       " 'scenes_',\n",
       " 'same_',\n",
       " 'he',\n",
       " 'end_',\n",
       " 'this',\n",
       " '... ',\n",
       " 'k_',\n",
       " 'while_',\n",
       " 'thing_',\n",
       " 'of',\n",
       " 'look_',\n",
       " 'quite_',\n",
       " 'out',\n",
       " 'lot_',\n",
       " 'want_',\n",
       " 'why_',\n",
       " 'seems_',\n",
       " 'every_',\n",
       " 'll_',\n",
       " 'pretty_',\n",
       " 'got_',\n",
       " 'able_',\n",
       " 'nothing_',\n",
       " 'good',\n",
       " 'As_',\n",
       " 'story',\n",
       " ' & ',\n",
       " 'another_',\n",
       " 'take_',\n",
       " 'to',\n",
       " 'years_',\n",
       " 'between_',\n",
       " 'give_',\n",
       " 'am_',\n",
       " 'work_',\n",
       " 'isn',\n",
       " 'part_',\n",
       " 'before_',\n",
       " 'actors_',\n",
       " 'may_',\n",
       " 'gets_',\n",
       " 'young_',\n",
       " 'down_',\n",
       " 'around_',\n",
       " 'ng',\n",
       " 'thought_',\n",
       " 'though_',\n",
       " 'end',\n",
       " 'without_',\n",
       " 'What_',\n",
       " 'They_',\n",
       " 'things_',\n",
       " 'life',\n",
       " 'always_',\n",
       " 'must_',\n",
       " 'cast_',\n",
       " 'almost_',\n",
       " 'h_',\n",
       " '10',\n",
       " 'saw_',\n",
       " 'own_',\n",
       " 'here',\n",
       " 'bit_',\n",
       " 'come_',\n",
       " 'both_',\n",
       " 'might_',\n",
       " 'g_',\n",
       " 'whole_',\n",
       " 'new_',\n",
       " 'director_',\n",
       " 'them',\n",
       " 'horror_',\n",
       " 'ce',\n",
       " 'You_',\n",
       " 'least_',\n",
       " 'bad',\n",
       " 'big_',\n",
       " 'enough_',\n",
       " 'him',\n",
       " 'feel_',\n",
       " 'probably_',\n",
       " 'up',\n",
       " 'here_',\n",
       " 'making_',\n",
       " 'long_',\n",
       " 'her',\n",
       " 'st_',\n",
       " 'kind_',\n",
       " '--',\n",
       " 'original_',\n",
       " 'fact_',\n",
       " 'rather_',\n",
       " 'or',\n",
       " 'far_',\n",
       " 'nt_',\n",
       " 'played_',\n",
       " 'found_',\n",
       " 'last_',\n",
       " 'movies',\n",
       " 'When_',\n",
       " 'so',\n",
       " '\", ',\n",
       " 'comes_',\n",
       " 'action_',\n",
       " 'She_',\n",
       " 've',\n",
       " 'our_',\n",
       " 'anything_',\n",
       " 'funny_',\n",
       " 'ion',\n",
       " 'right_',\n",
       " 'way',\n",
       " 'trying_',\n",
       " 'now_',\n",
       " 'ous_',\n",
       " 'each_',\n",
       " 'done_',\n",
       " 'since_',\n",
       " 'ic_',\n",
       " 'point_',\n",
       " '\". ',\n",
       " 'wasn',\n",
       " 'interesting_',\n",
       " 'c_',\n",
       " 'worst_',\n",
       " 'te_',\n",
       " 'le_',\n",
       " 'ble_',\n",
       " 'ty_',\n",
       " 'looks_',\n",
       " 'show',\n",
       " 'put_',\n",
       " 'looking_',\n",
       " 'especially_',\n",
       " 'believe_',\n",
       " 'en_',\n",
       " 'goes_',\n",
       " 'over',\n",
       " 'ce_',\n",
       " 'p_',\n",
       " 'films',\n",
       " 'hard_',\n",
       " 'main_',\n",
       " 'be',\n",
       " 'having_',\n",
       " 'ry',\n",
       " 'TV_',\n",
       " 'worth_',\n",
       " 'One_',\n",
       " 'do',\n",
       " 'al',\n",
       " 're',\n",
       " 'again',\n",
       " 'series_',\n",
       " 'takes_',\n",
       " 'guy_',\n",
       " 'family_',\n",
       " 'seem_',\n",
       " 'plays_',\n",
       " 'role_',\n",
       " 'away_',\n",
       " 'world_',\n",
       " 'My_',\n",
       " 'character',\n",
       " ', \"',\n",
       " 'performance_',\n",
       " '2_',\n",
       " 'So_',\n",
       " 'watched_',\n",
       " 'John_',\n",
       " 'th_',\n",
       " 'plot',\n",
       " 'script_',\n",
       " 'For_',\n",
       " 'sure_',\n",
       " 'characters',\n",
       " 'set_',\n",
       " 'different_',\n",
       " 'minutes_',\n",
       " 'All_',\n",
       " 'American_',\n",
       " 'anyone_',\n",
       " 'Not_',\n",
       " 'music_',\n",
       " 'ry_',\n",
       " 'shows_',\n",
       " 'too',\n",
       " 'son_',\n",
       " 'en',\n",
       " 'day_',\n",
       " 'use_',\n",
       " 'someone_',\n",
       " 'for',\n",
       " 'woman_',\n",
       " 'yet_',\n",
       " '.\" ',\n",
       " 'during_',\n",
       " 'she',\n",
       " 'ro',\n",
       " '- ',\n",
       " 'times_',\n",
       " 'left_',\n",
       " 'used_',\n",
       " 'le',\n",
       " 'three_',\n",
       " 'play_',\n",
       " 'work',\n",
       " 'ness_',\n",
       " 'We_',\n",
       " 'girl_',\n",
       " 'comedy_',\n",
       " 'ment_',\n",
       " 'an',\n",
       " 'simply_',\n",
       " 'off',\n",
       " 'ies_',\n",
       " 'funny',\n",
       " 'ne',\n",
       " 'acting',\n",
       " 'That_',\n",
       " 'fun_',\n",
       " 'completely_',\n",
       " 'st',\n",
       " 'seeing_',\n",
       " 'us',\n",
       " 'te',\n",
       " 'special_',\n",
       " 'ation_',\n",
       " 'as',\n",
       " 'ive_',\n",
       " 'ful_',\n",
       " 'read_',\n",
       " 'reason_',\n",
       " 'co',\n",
       " 'need_',\n",
       " 'sa',\n",
       " 'true_',\n",
       " 'ted_',\n",
       " 'like',\n",
       " 'ck',\n",
       " 'place_',\n",
       " 'they',\n",
       " '10_',\n",
       " 'However',\n",
       " 'until_',\n",
       " 'rest_',\n",
       " 'sense_',\n",
       " 'ity_',\n",
       " 'everything_',\n",
       " 'people',\n",
       " 'nt',\n",
       " 'ending_',\n",
       " 'again_',\n",
       " 'ers_',\n",
       " 'given_',\n",
       " 'idea_',\n",
       " 'let_',\n",
       " 'nice_',\n",
       " 'help_',\n",
       " 'no',\n",
       " 'truly_',\n",
       " 'beautiful_',\n",
       " 'ter',\n",
       " 'ck_',\n",
       " 'version_',\n",
       " 'try_',\n",
       " 'came_',\n",
       " 'Even_',\n",
       " 'DVD_',\n",
       " 'se',\n",
       " 'mis',\n",
       " 'scene',\n",
       " 'job_',\n",
       " 'ting_',\n",
       " 'Me',\n",
       " 'At_',\n",
       " 'who',\n",
       " 'money_',\n",
       " 'ment',\n",
       " 'ch',\n",
       " 'recommend_',\n",
       " 'was',\n",
       " 'once_',\n",
       " 'getting_',\n",
       " 'tell_',\n",
       " 'de_',\n",
       " 'gives_',\n",
       " 'not',\n",
       " 'Lo',\n",
       " 'we',\n",
       " 'son',\n",
       " 'shot_',\n",
       " 'second_',\n",
       " 'After_',\n",
       " 'To_',\n",
       " 'high_',\n",
       " 'screen_',\n",
       " ' -- ',\n",
       " 'keep_',\n",
       " 'felt_',\n",
       " 'with',\n",
       " 'great',\n",
       " 'everyone_',\n",
       " 'although_',\n",
       " 'poor_',\n",
       " 'el',\n",
       " 'half_',\n",
       " 'playing_',\n",
       " 'couple_',\n",
       " 'now',\n",
       " 'ble',\n",
       " 'excellent_',\n",
       " 'enjoy_',\n",
       " 'couldn',\n",
       " 'x_',\n",
       " 'ne_',\n",
       " ',\" ',\n",
       " 'ie_',\n",
       " 'go',\n",
       " 'become_',\n",
       " 'less',\n",
       " 'himself_',\n",
       " 'supposed_',\n",
       " 'won',\n",
       " 'understand_',\n",
       " 'seen',\n",
       " 'ally_',\n",
       " 'THE_',\n",
       " 'se_',\n",
       " 'actor_',\n",
       " 'ts_',\n",
       " 'small_',\n",
       " 'line_',\n",
       " 'na',\n",
       " 'audience_',\n",
       " 'fan_',\n",
       " 'et',\n",
       " 'world',\n",
       " 'entire_',\n",
       " 'said_',\n",
       " 'at',\n",
       " '3_',\n",
       " 'scenes',\n",
       " 'rs_',\n",
       " 'full_',\n",
       " 'year_',\n",
       " 'men_',\n",
       " 'ke',\n",
       " 'doing_',\n",
       " 'went_',\n",
       " 'director',\n",
       " 'back',\n",
       " 'early_',\n",
       " 'Hollywood_',\n",
       " 'start_',\n",
       " 'liked_',\n",
       " 'against_',\n",
       " 'remember_',\n",
       " 'love',\n",
       " 'He',\n",
       " 'along_',\n",
       " 'ic',\n",
       " 'His_',\n",
       " 'wife_',\n",
       " 'effects_',\n",
       " 'together_',\n",
       " 'ch_',\n",
       " 'Ra',\n",
       " 'ty',\n",
       " 'maybe_',\n",
       " 'age',\n",
       " 'S_',\n",
       " 'While_',\n",
       " 'often_',\n",
       " 'sort_',\n",
       " 'definitely_',\n",
       " 'No',\n",
       " 'script',\n",
       " 'times',\n",
       " 'absolutely_',\n",
       " 'book_',\n",
       " 'day',\n",
       " 'human_',\n",
       " 'There',\n",
       " 'top_',\n",
       " 'ta',\n",
       " 'becomes_',\n",
       " 'piece_',\n",
       " 'waste_',\n",
       " 'seemed_',\n",
       " 'down',\n",
       " '5_',\n",
       " 'later_',\n",
       " 'rs',\n",
       " 'ja',\n",
       " 'certainly_',\n",
       " 'budget_',\n",
       " 'th',\n",
       " 'nce_',\n",
       " '200',\n",
       " '. (',\n",
       " 'age_',\n",
       " 'next_',\n",
       " 'ar',\n",
       " 'several_',\n",
       " 'ling_',\n",
       " 'short_',\n",
       " 'sh',\n",
       " 'fe',\n",
       " 'Of_',\n",
       " 'instead_',\n",
       " 'Man',\n",
       " 'T_',\n",
       " 'right',\n",
       " 'father_',\n",
       " 'actors',\n",
       " 'wanted_',\n",
       " 'cast',\n",
       " 'black_',\n",
       " 'Don',\n",
       " 'more',\n",
       " '1_',\n",
       " 'comedy',\n",
       " 'better',\n",
       " 'camera_',\n",
       " 'wonderful_',\n",
       " 'production_',\n",
       " 'inter',\n",
       " 'course',\n",
       " 'low_',\n",
       " 'else_',\n",
       " 'w_',\n",
       " 'ness',\n",
       " 'course_',\n",
       " 'based_',\n",
       " 'ti',\n",
       " 'Some_',\n",
       " 'know',\n",
       " 'house_',\n",
       " 'say',\n",
       " 'de',\n",
       " 'watch',\n",
       " 'ous',\n",
       " 'pro',\n",
       " 'tries_',\n",
       " 'ra',\n",
       " 'kids_',\n",
       " 'etc',\n",
       " ' \\x96 ',\n",
       " 'loved_',\n",
       " 'est_',\n",
       " 'fun',\n",
       " 'made',\n",
       " 'video_',\n",
       " 'un',\n",
       " 'totally_',\n",
       " 'Michael_',\n",
       " 'ho',\n",
       " 'mind_',\n",
       " 'No_',\n",
       " 'Be',\n",
       " 'ive',\n",
       " 'La',\n",
       " 'Fi',\n",
       " 'du',\n",
       " 'ers',\n",
       " 'Well',\n",
       " 'wants_',\n",
       " 'How_',\n",
       " 'series',\n",
       " 'performances_',\n",
       " 'written_',\n",
       " 'live_',\n",
       " 'New_',\n",
       " 'So',\n",
       " 'Ne',\n",
       " 'Na',\n",
       " 'night_',\n",
       " 'ge',\n",
       " 'gave_',\n",
       " 'home_',\n",
       " 'heart',\n",
       " 'women_',\n",
       " 'nu',\n",
       " 'ss_',\n",
       " 'hope_',\n",
       " 'ci',\n",
       " 'friends_',\n",
       " 'Se',\n",
       " 'years',\n",
       " 'sub',\n",
       " 'head_',\n",
       " 'Y_',\n",
       " 'Du',\n",
       " '. \"',\n",
       " 'turn_',\n",
       " 'red_',\n",
       " 'perfect_',\n",
       " 'already_',\n",
       " 'classic_',\n",
       " 'tri',\n",
       " 'ss',\n",
       " 'person_',\n",
       " 'star_',\n",
       " 'screen',\n",
       " 'style_',\n",
       " 'ur',\n",
       " 'starts_',\n",
       " 'under_',\n",
       " 'Then_',\n",
       " 'ke_',\n",
       " 'ine',\n",
       " 'ies',\n",
       " 'um',\n",
       " 'ie',\n",
       " 'face_',\n",
       " 'ir',\n",
       " 'enjoyed_',\n",
       " 'point',\n",
       " 'lines_',\n",
       " 'Mr',\n",
       " 'turns_',\n",
       " 'what',\n",
       " 'side_',\n",
       " 'sex_',\n",
       " 'Ha',\n",
       " 'final_',\n",
       " ').<',\n",
       " 'With_',\n",
       " 'care_',\n",
       " 'tion_',\n",
       " 'She',\n",
       " 'ation',\n",
       " 'Ar',\n",
       " 'ma',\n",
       " 'problem_',\n",
       " 'lost_',\n",
       " 'are',\n",
       " 'li',\n",
       " '4_',\n",
       " 'fully_',\n",
       " 'oo',\n",
       " 'sha',\n",
       " 'Just_',\n",
       " 'name_',\n",
       " 'ina',\n",
       " 'boy_',\n",
       " 'finally_',\n",
       " 'ol',\n",
       " '!<',\n",
       " 'Bo',\n",
       " 'about',\n",
       " 'though',\n",
       " 'hand',\n",
       " 'ton',\n",
       " 'lead_',\n",
       " 'school_',\n",
       " 'ns',\n",
       " 'ha',\n",
       " 'favorite_',\n",
       " 'stupid_',\n",
       " 'gi',\n",
       " 'original',\n",
       " 'mean_',\n",
       " 'To',\n",
       " 'took_',\n",
       " 'either_',\n",
       " 'ni',\n",
       " 'book',\n",
       " 'episode_',\n",
       " 'om',\n",
       " 'Su',\n",
       " 'D_',\n",
       " 'Mc',\n",
       " 'house',\n",
       " 'cannot_',\n",
       " 'stars_',\n",
       " 'behind_',\n",
       " 'see',\n",
       " 'other',\n",
       " 'Che',\n",
       " 'role',\n",
       " 'art',\n",
       " 'ever',\n",
       " 'Why_',\n",
       " 'father',\n",
       " 'case_',\n",
       " 'tic_',\n",
       " 'moments_',\n",
       " 'Co',\n",
       " 'works_',\n",
       " 'sound_',\n",
       " 'Ta',\n",
       " 'guess_',\n",
       " 'perhaps_',\n",
       " 'Vi',\n",
       " 'thing',\n",
       " 'fine_',\n",
       " 'fact',\n",
       " 'music',\n",
       " 'non',\n",
       " 'ful',\n",
       " 'action',\n",
       " 'ity',\n",
       " 'ct',\n",
       " 'ate_',\n",
       " 'type_',\n",
       " 'lack_',\n",
       " 'death_',\n",
       " 'art_',\n",
       " 'able',\n",
       " 'Ja',\n",
       " 'ge_',\n",
       " 'wouldn',\n",
       " 'am',\n",
       " 'tor',\n",
       " 'extremely_',\n",
       " 'pre',\n",
       " 'self',\n",
       " 'Mor',\n",
       " 'particularly_',\n",
       " 'bo',\n",
       " 'est',\n",
       " 'Ba',\n",
       " 'ya',\n",
       " 'play',\n",
       " 'Pa',\n",
       " 'ther',\n",
       " 'heard_',\n",
       " 'however',\n",
       " 'ver',\n",
       " 'dy_',\n",
       " 'Sa',\n",
       " 'ding_',\n",
       " 'led_',\n",
       " 'late_',\n",
       " 'feeling_',\n",
       " 'per',\n",
       " 'low',\n",
       " 'ably_',\n",
       " 'Un',\n",
       " 'On_',\n",
       " 'known_',\n",
       " 'kill_',\n",
       " 'fight_',\n",
       " 'beginning_',\n",
       " 'cat',\n",
       " 'bit',\n",
       " 'title_',\n",
       " 'vo',\n",
       " 'short',\n",
       " 'old',\n",
       " 'including_',\n",
       " 'Da',\n",
       " 'coming_',\n",
       " 'That',\n",
       " 'place',\n",
       " 'looked_',\n",
       " 'best',\n",
       " 'Lu',\n",
       " 'ent_',\n",
       " 'bla',\n",
       " 'quality_',\n",
       " 'except_',\n",
       " '...<',\n",
       " 'ff',\n",
       " 'decent_',\n",
       " 'much',\n",
       " 'De',\n",
       " 'Bu',\n",
       " 'ter_',\n",
       " 'attempt_',\n",
       " 'Bi',\n",
       " 'taking_',\n",
       " 'ig',\n",
       " 'Ti',\n",
       " 'whose_',\n",
       " 'dialogue_',\n",
       " 'zz',\n",
       " 'war_',\n",
       " 'ill',\n",
       " 'Te',\n",
       " 'war',\n",
       " 'Hu',\n",
       " 'James_',\n",
       " '..',\n",
       " 'under',\n",
       " 'ring_',\n",
       " 'pa',\n",
       " 'ot',\n",
       " 'expect_',\n",
       " 'Ga',\n",
       " 'itself_',\n",
       " 'line',\n",
       " 'lives_',\n",
       " 'let',\n",
       " 'Dr',\n",
       " 'mp',\n",
       " 'che',\n",
       " 'mean',\n",
       " 'called_',\n",
       " 'complete_',\n",
       " 'terrible_',\n",
       " 'boring_',\n",
       " 'others_',\n",
       " '\" (',\n",
       " 'aren',\n",
       " 'star',\n",
       " 'long',\n",
       " 'Li',\n",
       " 'mother_',\n",
       " 'si',\n",
       " 'highly_',\n",
       " 'ab',\n",
       " 'ex',\n",
       " 'os',\n",
       " 'nd',\n",
       " 'ten_',\n",
       " 'ten',\n",
       " 'run_',\n",
       " 'directed_',\n",
       " 'town_',\n",
       " 'friend_',\n",
       " 'David_',\n",
       " 'taken_',\n",
       " 'finds_',\n",
       " 'fans_',\n",
       " 'Mar',\n",
       " 'writing_',\n",
       " 'white_',\n",
       " 'u_',\n",
       " 'obviously_',\n",
       " 'mar',\n",
       " 'Ho',\n",
       " 'year',\n",
       " 'stop_',\n",
       " 'f_',\n",
       " 'leave_',\n",
       " 'king_',\n",
       " 'act_',\n",
       " 'mind',\n",
       " 'entertaining_',\n",
       " 'ish_',\n",
       " 'Ka',\n",
       " 'throughout_',\n",
       " 'viewer_',\n",
       " 'despite_',\n",
       " 'Robert_',\n",
       " 'somewhat_',\n",
       " 'hour_',\n",
       " 'car_',\n",
       " 'evil_',\n",
       " 'Although_',\n",
       " 'wrong_',\n",
       " 'Ro',\n",
       " 'dead_',\n",
       " 'body_',\n",
       " 'awful_',\n",
       " 'home',\n",
       " 'exactly_',\n",
       " 'bi',\n",
       " 'family',\n",
       " 'ts',\n",
       " 'usually_',\n",
       " 'told_',\n",
       " 'z_',\n",
       " 'oc',\n",
       " 'minutes',\n",
       " 'tra',\n",
       " 'some',\n",
       " 'actor',\n",
       " 'den',\n",
       " 'but',\n",
       " 'Sha',\n",
       " 'tu',\n",
       " 'strong_',\n",
       " ...]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info.features['text'].encoder.subwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# type of 'train_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.data.ops.dataset_ops.DatasetV1Adapter"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# see what is in train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[  62    9    4  301 4161  267  148    1 3240 1779  787    3   62 2315\n",
      "  260 7968   21 1240   20  445   20  261  204    2    5   15  635 7742\n",
      "  149   97  101   25  184 4127    3 4666 7913  690   25    9  176    1\n",
      "  175  233   60   14  694    2   26   30 1858 3162   34    9 3636   40\n",
      "  267   11   14 2362 8050    3   19  695   29   51 1816 7138    2   26\n",
      "   14  101    1  397 3953  199  615    3   19  328    9 3362 4712 7961\n",
      "    5    1  184    9   77 4167   64 1152    2   55   12  459 1461    6\n",
      " 1713 3326   11 1147    7 1464 5691 7961  421 8026   38 1746 1074  618\n",
      "   54   65    3 1987    2    1 3326   29  214    5  318 2338 3960 8039\n",
      "    2    5  325    2   14   32    9 4157 7961   44  883 7975], shape=(138,), dtype=int64)\n",
      "tf.Tensor(0, shape=(), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "for x,y in train_data:\n",
    "    \n",
    "    print(x)\n",
    "    print(y)\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# types contained in train_data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n"
     ]
    }
   ],
   "source": [
    "for x,y in train_data:\n",
    "    \n",
    "    print(type(x))\n",
    "    print(type(y))\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# this is how to take the array in an EagerTensor element in train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  62    9    4  301 4161  267  148    1 3240 1779  787    3   62 2315\n",
      "  260 7968   21 1240   20  445   20  261  204    2    5   15  635 7742\n",
      "  149   97  101   25  184 4127    3 4666 7913  690   25    9  176    1\n",
      "  175  233   60   14  694    2   26   30 1858 3162   34    9 3636   40\n",
      "  267   11   14 2362 8050    3   19  695   29   51 1816 7138    2   26\n",
      "   14  101    1  397 3953  199  615    3   19  328    9 3362 4712 7961\n",
      "    5    1  184    9   77 4167   64 1152    2   55   12  459 1461    6\n",
      " 1713 3326   11 1147    7 1464 5691 7961  421 8026   38 1746 1074  618\n",
      "   54   65    3 1987    2    1 3326   29  214    5  318 2338 3960 8039\n",
      "    2    5  325    2   14   32    9 4157 7961   44  883 7975]\n"
     ]
    }
   ],
   "source": [
    "for x,y in train_data:\n",
    "    \n",
    "    print(x.numpy())\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# take the text sequences (integer encoded sequences) in train_data into a list\n",
    "# also take the labels into a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text_sequences = list()\n",
    "train_labels = list()\n",
    "for x,y in train_data:\n",
    "    train_text_sequences.append(x.numpy())\n",
    "    train_labels.append(y.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# take the text sequences list and labels list into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dct_train_labeled_text = {'text sequences': train_text_sequences, 'labels': train_labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_labeled_text = pd.DataFrame(dct_train_labeled_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text sequences</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>[62, 9, 4, 301, 4161, 267, 148, 1, 3240, 1779,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>[2130, 99, 12, 18, 55, 2554, 2, 3508, 5, 7995,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>[4491, 40, 6, 1, 7450, 34, 4798, 80, 4, 238, 7...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>[398, 105, 14, 9, 4, 98, 13, 732, 22, 63, 333,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>[62, 9, 33, 4, 132, 65, 3, 69, 2494, 1, 293, 5...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      text sequences  labels\n",
       "0  [62, 9, 4, 301, 4161, 267, 148, 1, 3240, 1779,...       0\n",
       "1  [2130, 99, 12, 18, 55, 2554, 2, 3508, 5, 7995,...       0\n",
       "2  [4491, 40, 6, 1, 7450, 34, 4798, 80, 4, 238, 7...       1\n",
       "3  [398, 105, 14, 9, 4, 98, 13, 732, 22, 63, 333,...       0\n",
       "4  [62, 9, 33, 4, 132, 65, 3, 69, 2494, 1, 293, 5...       1"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_labeled_text.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# the text sequences are of variable length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_of_train_text = list()\n",
    "for i,r in df_train_labeled_text.iterrows():\n",
    "    length_of_train_text.append(len(r['text sequences']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[138, 200, 708, 146, 126]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "length_of_train_text[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3944"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(length_of_train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(length_of_train_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# take the text sequences and labels in the test_data into lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text_sequences = list()\n",
    "test_labels = list()\n",
    "for x,y in test_data:\n",
    "    test_text_sequences.append(x.numpy())\n",
    "    test_labels.append(y.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# convert these lists into a dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dct_test_labeled_text = {'text sequences': test_text_sequences, 'labels': test_labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_labeled_text = pd.DataFrame(dct_test_labeled_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text sequences</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>[69, 5680, 22, 155, 6819, 7961, 6197, 309, 215...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>[12, 582, 448, 14, 44, 82, 1080, 2667, 464, 44...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>[62, 631, 32, 620, 3783, 8, 84, 3877, 190, 3, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>[6270, 762, 21, 5290, 6724, 3077, 8, 11, 59, 4...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>[8002, 7968, 111, 81, 33, 215, 7, 613, 82, 101...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      text sequences  labels\n",
       "0  [69, 5680, 22, 155, 6819, 7961, 6197, 309, 215...       1\n",
       "1  [12, 582, 448, 14, 44, 82, 1080, 2667, 464, 44...       0\n",
       "2  [62, 631, 32, 620, 3783, 8, 84, 3877, 190, 3, ...       1\n",
       "3  [6270, 762, 21, 5290, 6724, 3077, 8, 11, 59, 4...       0\n",
       "4  [8002, 7968, 111, 81, 33, 215, 7, 613, 82, 101...       0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_labeled_text.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# length of text sequences in test_data are of variable length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_of_test_text = list()\n",
    "for i,r in df_test_labeled_text.iterrows():\n",
    "    length_of_test_text.append(len(r['text sequences']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[226, 248, 203, 163, 159]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "length_of_test_text[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3454"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(length_of_test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(length_of_test_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# take the train text sequences and test text sequences as lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = list(df_train_labeled_text['text sequences'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = list(df_test_labeled_text['text sequences'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pad the text sequences\n",
    "# add 0s to text sequences having length less than the max length\n",
    "# truncate the text sequences having length more than the max length to max length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 100\n",
    "\n",
    "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
    "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# after padding see the lengths of text sequences in train and test data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_post_padding_length = list()\n",
    "for a in X_train:\n",
    "    train_post_padding_length.append(len(a))\n",
    "    \n",
    "test_post_padding_length = list()\n",
    "for a in X_test:\n",
    "    test_post_padding_length.append(len(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[100, 100, 100, 100, 100]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_post_padding_length[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[100, 100, 100, 100, 100]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_post_padding_length[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build the neural network model_1 - global average pooling 1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'keras' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-a6459c5eab84>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0membedding_dim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m model_1 = keras.Sequential([\n\u001b[0m\u001b[0;32m      4\u001b[0m   \u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0membedding_dim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m   \u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGlobalAveragePooling1D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'keras' is not defined"
     ]
    }
   ],
   "source": [
    "embedding_dim=16\n",
    "\n",
    "model_1 = keras.Sequential([\n",
    "  layers.Embedding(encoder.vocab_size, embedding_dim),\n",
    "  layers.GlobalAveragePooling1D(),\n",
    "  layers.Dense(16, activation='relu'),\n",
    "  layers.Dense(1)\n",
    "])\n",
    "\n",
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# take the train labels as a numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = list(df_train_labeled_text['labels'])\n",
    "y_train = np.array(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# take the test labels as a numpy array "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = list(df_test_labeled_text['labels'])\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# compile the model_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train the model_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_1 = model_1.fit(X_train, y_train, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# evaluate the trained model_1 on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build the neural network model_2 - lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "embedding_dim=16\n",
    "\n",
    "model_2 = keras.Sequential([\n",
    "  layers.Embedding(encoder.vocab_size, embedding_dim),\n",
    "  layers.LSTM(128),\n",
    "  layers.Dense(16, activation='relu'),\n",
    "  layers.Dense(1)\n",
    "])\n",
    "\n",
    "model_2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# compile the model_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train the model_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history_2 = model_2.fit(X_train, y_train, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# evaluate the model_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
